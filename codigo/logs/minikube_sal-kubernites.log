* 
* ==> Audit <==
* |---------|--------------------------------|----------|------|---------|-------------------------------|-------------------------------|
| Command |              Args              | Profile  | User | Version |          Start Time           |           End Time            |
|---------|--------------------------------|----------|------|---------|-------------------------------|-------------------------------|
| kubectl | -- get svc                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 01:37:14 CET | Sun, 28 Nov 2021 01:37:15 CET |
| kubectl | -- describe svc netdata        | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 01:37:47 CET | Sun, 28 Nov 2021 01:37:53 CET |
| kubectl | -- get svc netdata             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 01:54:26 CET | Sun, 28 Nov 2021 01:54:29 CET |
| kubectl | -- get svc netdata -o wide     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 01:54:35 CET | Sun, 28 Nov 2021 01:54:35 CET |
| kubectl | -- port-forward -h             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 01:55:15 CET | Sun, 28 Nov 2021 01:55:15 CET |
| kubectl | -- port-forward svc/netdata    | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 01:58:20 CET | Sun, 28 Nov 2021 02:14:55 CET |
|         | :19999                         |          |      |         |                               |                               |
| stop    |                                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 02:16:51 CET | Sun, 28 Nov 2021 02:17:08 CET |
| start   |                                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:31:11 CET | Sun, 28 Nov 2021 12:31:52 CET |
| start   | --memory=2200mb                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:35:37 CET | Sun, 28 Nov 2021 12:36:27 CET |
| kubectl | -- get svc                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:37:56 CET | Sun, 28 Nov 2021 12:37:57 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:38:06 CET | Sun, 28 Nov 2021 12:38:06 CET |
| stop    |                                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:38:19 CET | Sun, 28 Nov 2021 12:38:35 CET |
| start   |                                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:38:43 CET | Sun, 28 Nov 2021 12:39:38 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:39:51 CET | Sun, 28 Nov 2021 12:39:52 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:39:58 CET | Sun, 28 Nov 2021 12:39:59 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:01 CET | Sun, 28 Nov 2021 12:40:01 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:06 CET | Sun, 28 Nov 2021 12:40:06 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:07 CET | Sun, 28 Nov 2021 12:40:07 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:09 CET | Sun, 28 Nov 2021 12:40:09 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:10 CET | Sun, 28 Nov 2021 12:40:10 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:15 CET | Sun, 28 Nov 2021 12:40:15 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:20 CET | Sun, 28 Nov 2021 12:40:20 CET |
| kubectl | -- get svc,pods                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:24 CET | Sun, 28 Nov 2021 12:40:25 CET |
| kubectl | -- port-forward -h             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 12:40:59 CET | Sun, 28 Nov 2021 12:40:59 CET |
| start   |                                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:14:47 CET | Sun, 28 Nov 2021 13:18:06 CET |
| kubectl | -- get pods                    | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:19:06 CET | Sun, 28 Nov 2021 13:19:07 CET |
| kubectl | -- cluster-info                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:19:14 CET | Sun, 28 Nov 2021 13:19:14 CET |
| kubectl | -- get secret                  | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:19:31 CET | Sun, 28 Nov 2021 13:19:31 CET |
| kubectl | -- get ep                      | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:19:44 CET | Sun, 28 Nov 2021 13:19:44 CET |
| kubectl | -- get pods                    | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:20:02 CET | Sun, 28 Nov 2021 13:20:02 CET |
| kubectl | -- cluster-info                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:20:08 CET | Sun, 28 Nov 2021 13:20:08 CET |
| kubectl | -- port-forward -h             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:20:43 CET | Sun, 28 Nov 2021 13:20:43 CET |
| kubectl | -- cluster-info                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:21:47 CET | Sun, 28 Nov 2021 13:21:48 CET |
| kubectl | -- port-forward -h             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:21:50 CET | Sun, 28 Nov 2021 13:21:51 CET |
| kubectl | -- get svc                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:22:01 CET | Sun, 28 Nov 2021 13:22:01 CET |
| kubectl | -- cluster-info                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:22:59 CET | Sun, 28 Nov 2021 13:22:59 CET |
| kubectl | -- get svc                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:23:01 CET | Sun, 28 Nov 2021 13:23:02 CET |
| kubectl | -- get pod                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:23:05 CET | Sun, 28 Nov 2021 13:23:05 CET |
| kubectl | -- port-forward -h             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:23:28 CET | Sun, 28 Nov 2021 13:23:28 CET |
| kubectl | -- edit svc/netdata            | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:23:58 CET | Sun, 28 Nov 2021 13:24:15 CET |
| kubectl | -- get svc                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:24:19 CET | Sun, 28 Nov 2021 13:24:20 CET |
| kubectl | -- edit svc/netdata            | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:36:01 CET | Sun, 28 Nov 2021 13:37:10 CET |
| kubectl | -- get svc                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:37:24 CET | Sun, 28 Nov 2021 13:37:24 CET |
| kubectl | -- get pod                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 13:38:11 CET | Sun, 28 Nov 2021 13:38:13 CET |
| stop    |                                | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 15:02:56 CET | Sun, 28 Nov 2021 15:03:20 CET |
| start   | --memory=2200                  | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:01:49 CET | Sun, 28 Nov 2021 20:08:49 CET |
| kubectl | -- logs -h                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:09:45 CET | Sun, 28 Nov 2021 20:09:45 CET |
| kubectl | -- get pods                    | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:11:29 CET | Sun, 28 Nov 2021 20:11:29 CET |
| kubectl | -- logs pod/sal-ubuntu         | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:12:15 CET | Sun, 28 Nov 2021 20:12:15 CET |
| kubectl | -- logs -h                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:12:21 CET | Sun, 28 Nov 2021 20:12:21 CET |
| kubectl | -- get pod                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:12:40 CET | Sun, 28 Nov 2021 20:12:41 CET |
| kubectl | -- logs sal-ubuntu             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:12:49 CET | Sun, 28 Nov 2021 20:12:50 CET |
| kubectl | -- delete pod                  | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:13:35 CET | Sun, 28 Nov 2021 20:13:40 CET |
|         | hello-node-7567d9fdc9-kbfc7    |          |      |         |                               |                               |
| kubectl | -- get pods                    | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:13:47 CET | Sun, 28 Nov 2021 20:13:47 CET |
| kubectl | -- get deploy                  | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:14:04 CET | Sun, 28 Nov 2021 20:14:05 CET |
| kubectl | -- delete deploy hello-node    | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:14:23 CET | Sun, 28 Nov 2021 20:14:24 CET |
| kubectl | -- get pod,deploy,svc          | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:14:32 CET | Sun, 28 Nov 2021 20:14:34 CET |
| kubectl | -- logs -h                     | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:15:22 CET | Sun, 28 Nov 2021 20:15:22 CET |
| kubectl | -- logs sal-ubuntu             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:16:00 CET | Sun, 28 Nov 2021 20:16:05 CET |
| kubectl | -- logs sal-ubuntu             | minikube | sal  | v1.24.0 | Sun, 28 Nov 2021 20:16:18 CET | Sun, 28 Nov 2021 20:16:18 CET |
|---------|--------------------------------|----------|------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2021/11/28 20:01:49
Running on machine: sal-kubernites
Binary: Built with gc go1.17.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1128 20:01:49.782501    3242 out.go:297] Setting OutFile to fd 1 ...
I1128 20:01:49.786518    3242 out.go:349] isatty.IsTerminal(1) = true
I1128 20:01:49.786524    3242 out.go:310] Setting ErrFile to fd 2...
I1128 20:01:49.786533    3242 out.go:349] isatty.IsTerminal(2) = true
I1128 20:01:49.786718    3242 root.go:313] Updating PATH: /home/sal/.minikube/bin
W1128 20:01:49.787152    3242 root.go:291] Error reading config file at /home/sal/.minikube/config/config.json: open /home/sal/.minikube/config/config.json: no such file or directory
I1128 20:01:49.788058    3242 out.go:304] Setting JSON to false
I1128 20:01:49.792037    3242 start.go:112] hostinfo: {"hostname":"sal-kubernites","uptime":87,"bootTime":1638126023,"procs":371,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.4.0-90-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"905ccc09-ee6a-46b0-89d7-b5a3606690ed"}
I1128 20:01:49.792135    3242 start.go:122] virtualization: kvm host
I1128 20:01:49.793925    3242 out.go:176] 😄  minikube v1.24.0 en Ubuntu 20.04
I1128 20:01:49.794204    3242 notify.go:174] Checking for updates...
I1128 20:01:49.800008    3242 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I1128 20:01:49.806533    3242 driver.go:343] Setting default libvirt URI to qemu:///system
I1128 20:01:50.993652    3242 docker.go:132] docker version: linux-20.10.11
I1128 20:01:51.000031    3242 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I1128 20:01:52.807067    3242 cli_runner.go:168] Completed: docker system info --format "{{json .}}": (1.807007568s)
I1128 20:01:52.807361    3242 info.go:263] docker info: {ID:M5UR:XRTB:2SC6:BTDP:D3VU:2HFW:2QWE:TJ6X:Z5NT:SGC4:4VGO:6R25 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2021-11-28 20:01:51.036623937 +0100 CET LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.4.0-90-generic OperatingSystem:Ubuntu 20.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3108339712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:sal-kubernites Labels:[] ExperimentalBuild:false ServerVersion:20.10.11 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No swap limit support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.9.0]] Warnings:<nil>}}
I1128 20:01:52.807432    3242 docker.go:237] overlay module found
I1128 20:01:52.808588    3242 out.go:176] ✨  Using the docker driver based on existing profile
I1128 20:01:52.808624    3242 start.go:280] selected driver: docker
I1128 20:01:52.808628    3242 start.go:762] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[Dashboard:kubernetesui/dashboard:v2.3.1@sha256:ec27f462cf1946220f5a9ace416a84a57c18f98c777876a8054405d1428cc92e IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 MetricsScraper:kubernetesui/metrics-scraper:v1.0.7@sha256:36d5b3f60e1a144cc5ada820910535074bdf5cf73fb70d1ff1681537eef4e172 MetricsServer:metrics-server/metrics-server:v0.4.2@sha256:dbc33d7d35d2a9cc5ab402005aa7a0d13be6192f3550c7d42cba8d2d5e3a5d62 Tiller:helm/tiller:v2.17.0@sha256:4c43eb385032945cad047d2350e4945d913b90b3ab43ee61cecb32a495c6df0f] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sal:/minikube-host}
I1128 20:01:52.808805    3242 start.go:773] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
W1128 20:01:52.808832    3242 oci.go:119] Your kernel does not support memory limit capabilities or the cgroup is not mounted.
W1128 20:01:52.808860    3242 out.go:241] ❗  Your cgroup does not allow setting memory.
I1128 20:01:52.809984    3242 out.go:176]     ▪ More information: https://docs.docker.com/engine/install/linux-postinstall/#your-kernel-does-not-support-cgroup-swap-limit-capabilities
I1128 20:01:52.811347    3242 out.go:176] 
W1128 20:01:52.811471    3242 out.go:241] 🧯  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2964MiB). You may face stability issues.
W1128 20:01:52.811611    3242 out.go:241] 💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'
I1128 20:01:52.812616    3242 out.go:176] 
I1128 20:01:52.812835    3242 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I1128 20:01:52.928542    3242 info.go:263] docker info: {ID:M5UR:XRTB:2SC6:BTDP:D3VU:2HFW:2QWE:TJ6X:Z5NT:SGC4:4VGO:6R25 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2021-11-28 20:01:52.847659917 +0100 CET LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.4.0-90-generic OperatingSystem:Ubuntu 20.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3108339712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:sal-kubernites Labels:[] ExperimentalBuild:false ServerVersion:20.10.11 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No swap limit support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.9.0]] Warnings:<nil>}}
W1128 20:01:52.928684    3242 oci.go:119] Your kernel does not support memory limit capabilities or the cgroup is not mounted.
W1128 20:01:52.928713    3242 out.go:241] ❗  Your cgroup does not allow setting memory.
I1128 20:01:52.930004    3242 out.go:176]     ▪ More information: https://docs.docker.com/engine/install/linux-postinstall/#your-kernel-does-not-support-cgroup-swap-limit-capabilities
I1128 20:01:52.931173    3242 out.go:176] 
W1128 20:01:52.931410    3242 out.go:241] 🧯  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2964MiB). You may face stability issues.
W1128 20:01:52.931512    3242 out.go:241] 💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'
I1128 20:01:52.932834    3242 out.go:176] 
I1128 20:01:52.934009    3242 out.go:176] 
W1128 20:01:52.934154    3242 out.go:241] 🧯  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2964MiB). You may face stability issues.
W1128 20:01:52.934271    3242 out.go:241] 💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'
I1128 20:01:52.935340    3242 out.go:176] 
I1128 20:01:52.935443    3242 cni.go:93] Creating CNI manager for ""
I1128 20:01:52.937021    3242 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1128 20:01:52.937047    3242 start_flags.go:282] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[Dashboard:kubernetesui/dashboard:v2.3.1@sha256:ec27f462cf1946220f5a9ace416a84a57c18f98c777876a8054405d1428cc92e IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 MetricsScraper:kubernetesui/metrics-scraper:v1.0.7@sha256:36d5b3f60e1a144cc5ada820910535074bdf5cf73fb70d1ff1681537eef4e172 MetricsServer:metrics-server/metrics-server:v0.4.2@sha256:dbc33d7d35d2a9cc5ab402005aa7a0d13be6192f3550c7d42cba8d2d5e3a5d62 Tiller:helm/tiller:v2.17.0@sha256:4c43eb385032945cad047d2350e4945d913b90b3ab43ee61cecb32a495c6df0f] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sal:/minikube-host}
I1128 20:01:52.938231    3242 out.go:176] 👍  Starting control plane node minikube in cluster minikube
I1128 20:01:52.942630    3242 cache.go:118] Beginning downloading kic base image for docker with docker
I1128 20:01:52.943732    3242 out.go:176] 🚜  Pulling base image ...
I1128 20:01:52.943822    3242 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I1128 20:01:52.943884    3242 preload.go:148] Found local preload: /home/sal/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4
I1128 20:01:52.943892    3242 cache.go:57] Caching tarball of preloaded images
I1128 20:01:52.944066    3242 preload.go:174] Found /home/sal/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1128 20:01:52.944080    3242 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.3 on docker
I1128 20:01:52.944083    3242 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I1128 20:01:52.944174    3242 profile.go:147] Saving config to /home/sal/.minikube/profiles/minikube/config.json ...
I1128 20:01:53.008068    3242 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I1128 20:01:53.008085    3242 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
I1128 20:01:53.008112    3242 cache.go:206] Successfully downloaded all kic artifacts
I1128 20:01:53.008138    3242 start.go:313] acquiring machines lock for minikube: {Name:mk15e986929af1704921382ffb1866326d6100db Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 20:01:53.008364    3242 start.go:317] acquired machines lock for "minikube" in 202.608µs
I1128 20:01:53.008378    3242 start.go:93] Skipping create...Using existing machine configuration
I1128 20:01:53.008381    3242 fix.go:55] fixHost starting: 
I1128 20:01:53.008551    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1128 20:01:53.142281    3242 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1128 20:01:53.142297    3242 fix.go:134] unexpected machine state, will restart: <nil>
I1128 20:01:53.143841    3242 out.go:176] 🔄  Restarting existing docker container for "minikube" ...
I1128 20:01:53.143951    3242 cli_runner.go:115] Run: docker start minikube
I1128 20:01:54.975999    3242 cli_runner.go:168] Completed: docker start minikube: (1.832027222s)
I1128 20:01:54.976070    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1128 20:01:55.024252    3242 kic.go:420] container "minikube" state is running.
I1128 20:01:55.024733    3242 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1128 20:01:55.069953    3242 profile.go:147] Saving config to /home/sal/.minikube/profiles/minikube/config.json ...
I1128 20:01:55.070320    3242 machine.go:88] provisioning docker machine ...
I1128 20:01:55.070339    3242 ubuntu.go:169] provisioning hostname "minikube"
I1128 20:01:55.070373    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:01:55.106993    3242 main.go:130] libmachine: Using SSH client type: native
I1128 20:01:55.109678    3242 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1128 20:01:55.109688    3242 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1128 20:01:55.110432    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59364->127.0.0.1:49157: read: connection reset by peer
I1128 20:01:58.112033    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59368->127.0.0.1:49157: read: connection reset by peer
I1128 20:02:01.114508    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1128 20:02:04.116521    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59376->127.0.0.1:49157: read: connection reset by peer
I1128 20:02:07.119108    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59390->127.0.0.1:49157: read: connection reset by peer
I1128 20:02:10.120027    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59394->127.0.0.1:49157: read: connection reset by peer
I1128 20:02:13.122492    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59398->127.0.0.1:49157: read: connection reset by peer
I1128 20:02:16.123087    3242 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59404->127.0.0.1:49157: read: connection reset by peer
I1128 20:02:19.437389    3242 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I1128 20:02:19.437471    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:19.498501    3242 main.go:130] libmachine: Using SSH client type: native
I1128 20:02:19.499461    3242 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1128 20:02:19.499473    3242 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1128 20:02:19.615379    3242 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1128 20:02:19.616963    3242 ubuntu.go:175] set auth options {CertDir:/home/sal/.minikube CaCertPath:/home/sal/.minikube/certs/ca.pem CaPrivateKeyPath:/home/sal/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/sal/.minikube/machines/server.pem ServerKeyPath:/home/sal/.minikube/machines/server-key.pem ClientKeyPath:/home/sal/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/sal/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/sal/.minikube}
I1128 20:02:19.617466    3242 ubuntu.go:177] setting up certificates
I1128 20:02:19.617477    3242 provision.go:83] configureAuth start
I1128 20:02:19.617534    3242 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1128 20:02:19.650727    3242 provision.go:138] copyHostCerts
I1128 20:02:19.656136    3242 exec_runner.go:144] found /home/sal/.minikube/ca.pem, removing ...
I1128 20:02:19.656158    3242 exec_runner.go:207] rm: /home/sal/.minikube/ca.pem
I1128 20:02:19.656946    3242 exec_runner.go:151] cp: /home/sal/.minikube/certs/ca.pem --> /home/sal/.minikube/ca.pem (1070 bytes)
I1128 20:02:19.658327    3242 exec_runner.go:144] found /home/sal/.minikube/cert.pem, removing ...
I1128 20:02:19.658336    3242 exec_runner.go:207] rm: /home/sal/.minikube/cert.pem
I1128 20:02:19.658373    3242 exec_runner.go:151] cp: /home/sal/.minikube/certs/cert.pem --> /home/sal/.minikube/cert.pem (1111 bytes)
I1128 20:02:19.659683    3242 exec_runner.go:144] found /home/sal/.minikube/key.pem, removing ...
I1128 20:02:19.659692    3242 exec_runner.go:207] rm: /home/sal/.minikube/key.pem
I1128 20:02:19.661022    3242 exec_runner.go:151] cp: /home/sal/.minikube/certs/key.pem --> /home/sal/.minikube/key.pem (1675 bytes)
I1128 20:02:19.662321    3242 provision.go:112] generating server cert: /home/sal/.minikube/machines/server.pem ca-key=/home/sal/.minikube/certs/ca.pem private-key=/home/sal/.minikube/certs/ca-key.pem org=sal.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1128 20:02:19.758019    3242 provision.go:172] copyRemoteCerts
I1128 20:02:19.759060    3242 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1128 20:02:19.759109    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:19.792567    3242 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/sal/.minikube/machines/minikube/id_rsa Username:docker}
I1128 20:02:19.878516    3242 ssh_runner.go:319] scp /home/sal/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I1128 20:02:19.914658    3242 ssh_runner.go:319] scp /home/sal/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1128 20:02:19.934301    3242 ssh_runner.go:319] scp /home/sal/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I1128 20:02:19.953874    3242 provision.go:86] duration metric: configureAuth took 336.388205ms
I1128 20:02:19.953886    3242 ubuntu.go:193] setting minikube options for container-runtime
I1128 20:02:19.955702    3242 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I1128 20:02:19.955741    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:19.987486    3242 main.go:130] libmachine: Using SSH client type: native
I1128 20:02:19.987590    3242 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1128 20:02:19.987595    3242 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1128 20:02:20.110668    3242 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I1128 20:02:20.110677    3242 ubuntu.go:71] root file system type: overlay
I1128 20:02:20.111227    3242 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1128 20:02:20.111269    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:20.163668    3242 main.go:130] libmachine: Using SSH client type: native
I1128 20:02:20.163862    3242 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1128 20:02:20.163915    3242 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1128 20:02:20.296778    3242 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1128 20:02:20.296829    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:20.334053    3242 main.go:130] libmachine: Using SSH client type: native
I1128 20:02:20.334180    3242 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a0280] 0x7a3360 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1128 20:02:20.334193    3242 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1128 20:02:20.463777    3242 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1128 20:02:20.463789    3242 machine.go:91] provisioned docker machine in 25.393459674s
I1128 20:02:20.463797    3242 start.go:267] post-start starting for "minikube" (driver="docker")
I1128 20:02:20.463801    3242 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1128 20:02:20.463872    3242 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1128 20:02:20.463915    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:20.500748    3242 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/sal/.minikube/machines/minikube/id_rsa Username:docker}
I1128 20:02:20.591825    3242 ssh_runner.go:152] Run: cat /etc/os-release
I1128 20:02:20.595948    3242 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1128 20:02:20.595977    3242 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1128 20:02:20.595982    3242 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1128 20:02:20.595990    3242 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I1128 20:02:20.596007    3242 filesync.go:126] Scanning /home/sal/.minikube/addons for local assets ...
I1128 20:02:20.596428    3242 filesync.go:126] Scanning /home/sal/.minikube/files for local assets ...
I1128 20:02:20.596824    3242 filesync.go:149] local asset: /home/sal/.minikube/files/00-tailscale.yaml -> 00-tailscale.yaml in /
I1128 20:02:20.596846    3242 ssh_runner.go:319] scp /home/sal/.minikube/files/00-tailscale.yaml --> /00-tailscale.yaml (161 bytes)
I1128 20:02:20.618838    3242 start.go:270] post-start completed in 155.029782ms
I1128 20:02:20.618913    3242 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1128 20:02:20.618940    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:20.653347    3242 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/sal/.minikube/machines/minikube/id_rsa Username:docker}
I1128 20:02:20.736113    3242 fix.go:57] fixHost completed within 27.727727199s
I1128 20:02:20.736143    3242 start.go:80] releasing machines lock for "minikube", held for 27.727773146s
I1128 20:02:20.736255    3242 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1128 20:02:20.769522    3242 ssh_runner.go:152] Run: systemctl --version
I1128 20:02:20.769613    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:20.770631    3242 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I1128 20:02:20.770698    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:02:20.822536    3242 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/sal/.minikube/machines/minikube/id_rsa Username:docker}
I1128 20:02:20.824282    3242 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/sal/.minikube/machines/minikube/id_rsa Username:docker}
I1128 20:02:20.923575    3242 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I1128 20:02:23.183564    3242 ssh_runner.go:192] Completed: sudo systemctl is-active --quiet service containerd: (2.259968838s)
I1128 20:02:23.183630    3242 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I1128 20:02:23.184405    3242 ssh_runner.go:192] Completed: curl -sS -m 2 https://k8s.gcr.io/: (2.413743039s)
W1128 20:02:23.185095    3242 start.go:664] [curl -sS -m 2 https://k8s.gcr.io/] failed: curl -sS -m 2 https://k8s.gcr.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
W1128 20:02:23.188290    3242 out.go:241] ❗  This container is having trouble accessing https://k8s.gcr.io
W1128 20:02:23.188351    3242 out.go:241] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1128 20:02:23.196679    3242 cruntime.go:255] skipping containerd shutdown because we are bound to it
I1128 20:02:23.197534    3242 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I1128 20:02:23.215524    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I1128 20:02:23.234766    3242 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I1128 20:02:23.339167    3242 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I1128 20:02:23.423305    3242 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I1128 20:02:23.435732    3242 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I1128 20:02:23.512656    3242 ssh_runner.go:152] Run: sudo systemctl start docker
I1128 20:02:23.523840    3242 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I1128 20:02:44.600586    3242 ssh_runner.go:192] Completed: docker version --format {{.Server.Version}}: (21.076717465s)
I1128 20:02:44.600623    3242 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I1128 20:02:44.658017    3242 out.go:203] 🐳  Preparando Kubernetes v1.22.3 en Docker 20.10.8...
I1128 20:02:44.664047    3242 cli_runner.go:115] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1128 20:02:44.706194    3242 ssh_runner.go:152] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1128 20:02:44.709979    3242 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1128 20:02:44.735316    3242 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I1128 20:02:44.735813    3242 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I1128 20:02:44.781047    3242 docker.go:558] Got preloaded images: -- stdout --
alpine:latest
bitnami/nginx-ingress-controller:1.1.0-debian-10-r0
bitnami/nginx:1.21.4-debian-10-r19
bitnami/prometheus-operator:0.52.1-debian-10-r0
bitnami/prometheus:2.31.1-debian-10-r10
bitnami/alertmanager:0.23.0-debian-10-r81
alpine:3.12
bitnami/kube-state-metrics:2.2.4-debian-10-r0
bitnami/cert-manager:1.6.1-debian-10-r0
bitnami/cert-manager-webhook:1.6.1-debian-10-r0
bitnami/cainjector:1.6.1-debian-10-r0
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
bitnami/node-exporter:1.2.2-debian-10-r78
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
ubuntu:latest
k8s.gcr.io/ingress-nginx/controller:<none>
k8s.gcr.io/ingress-nginx/kube-webhook-certgen:<none>
netdata/agent-sd:v0.2.2
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
netdata/netdata:v1.31.0
quay.io/prometheus/prometheus:v2.27.1
k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5
quay.io/prometheus/node-exporter:v1.1.2
k8s.gcr.io/metrics-server/metrics-server:<none>
ghcr.io/helm/tiller:<none>
k8s.gcr.io/echoserver:1.4

-- /stdout --
I1128 20:02:44.781057    3242 docker.go:489] Images already preloaded, skipping extraction
I1128 20:02:44.781577    3242 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I1128 20:02:44.832343    3242 docker.go:558] Got preloaded images: -- stdout --
alpine:latest
bitnami/nginx-ingress-controller:1.1.0-debian-10-r0
bitnami/nginx:1.21.4-debian-10-r19
bitnami/prometheus-operator:0.52.1-debian-10-r0
bitnami/prometheus:2.31.1-debian-10-r10
bitnami/alertmanager:0.23.0-debian-10-r81
alpine:3.12
bitnami/kube-state-metrics:2.2.4-debian-10-r0
bitnami/cert-manager:1.6.1-debian-10-r0
bitnami/cert-manager-webhook:1.6.1-debian-10-r0
bitnami/cainjector:1.6.1-debian-10-r0
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
bitnami/node-exporter:1.2.2-debian-10-r78
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
ubuntu:latest
k8s.gcr.io/ingress-nginx/controller:<none>
k8s.gcr.io/ingress-nginx/kube-webhook-certgen:<none>
netdata/agent-sd:v0.2.2
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
netdata/netdata:v1.31.0
quay.io/prometheus/prometheus:v2.27.1
k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5
quay.io/prometheus/node-exporter:v1.1.2
k8s.gcr.io/metrics-server/metrics-server:<none>
ghcr.io/helm/tiller:<none>
k8s.gcr.io/echoserver:1.4

-- /stdout --
I1128 20:02:44.832354    3242 cache_images.go:79] Images are preloaded, skipping loading
I1128 20:02:44.832427    3242 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I1128 20:02:45.886559    3242 ssh_runner.go:192] Completed: docker info --format {{.CgroupDriver}}: (1.054110757s)
I1128 20:02:45.890907    3242 cni.go:93] Creating CNI manager for ""
I1128 20:02:45.891082    3242 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1128 20:02:45.892007    3242 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1128 20:02:45.892034    3242 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1128 20:02:45.893906    3242 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1128 20:02:45.895491    3242 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1128 20:02:45.895691    3242 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I1128 20:02:45.922977    3242 binaries.go:44] Found k8s binaries, skipping transfer
I1128 20:02:45.923381    3242 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1128 20:02:45.938028    3242 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I1128 20:02:45.958129    3242 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1128 20:02:45.976033    3242 ssh_runner.go:319] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2051 bytes)
I1128 20:02:45.997362    3242 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1128 20:02:46.004702    3242 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1128 20:02:46.038503    3242 certs.go:54] Setting up /home/sal/.minikube/profiles/minikube for IP: 192.168.49.2
I1128 20:02:46.039606    3242 certs.go:182] skipping minikubeCA CA generation: /home/sal/.minikube/ca.key
I1128 20:02:46.040359    3242 certs.go:182] skipping proxyClientCA CA generation: /home/sal/.minikube/proxy-client-ca.key
I1128 20:02:46.040973    3242 certs.go:298] skipping minikube-user signed cert generation: /home/sal/.minikube/profiles/minikube/client.key
I1128 20:02:46.041743    3242 certs.go:298] skipping minikube signed cert generation: /home/sal/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1128 20:02:46.043661    3242 certs.go:298] skipping aggregator signed cert generation: /home/sal/.minikube/profiles/minikube/proxy-client.key
I1128 20:02:46.043873    3242 certs.go:388] found cert: /home/sal/.minikube/certs/home/sal/.minikube/certs/ca-key.pem (1679 bytes)
I1128 20:02:46.043905    3242 certs.go:388] found cert: /home/sal/.minikube/certs/home/sal/.minikube/certs/ca.pem (1070 bytes)
I1128 20:02:46.043923    3242 certs.go:388] found cert: /home/sal/.minikube/certs/home/sal/.minikube/certs/cert.pem (1111 bytes)
I1128 20:02:46.043939    3242 certs.go:388] found cert: /home/sal/.minikube/certs/home/sal/.minikube/certs/key.pem (1675 bytes)
I1128 20:02:46.054201    3242 ssh_runner.go:319] scp /home/sal/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1128 20:02:46.079816    3242 ssh_runner.go:319] scp /home/sal/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1128 20:02:46.099579    3242 ssh_runner.go:319] scp /home/sal/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1128 20:02:46.134025    3242 ssh_runner.go:319] scp /home/sal/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1128 20:02:46.161210    3242 ssh_runner.go:319] scp /home/sal/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1128 20:02:46.184611    3242 ssh_runner.go:319] scp /home/sal/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1128 20:02:46.211138    3242 ssh_runner.go:319] scp /home/sal/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1128 20:02:46.244158    3242 ssh_runner.go:319] scp /home/sal/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1128 20:02:46.269922    3242 ssh_runner.go:319] scp /home/sal/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1128 20:02:46.295200    3242 ssh_runner.go:319] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1128 20:02:46.317743    3242 ssh_runner.go:152] Run: openssl version
I1128 20:02:46.339100    3242 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1128 20:02:46.356069    3242 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1128 20:02:46.360856    3242 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Nov 25 21:57 /usr/share/ca-certificates/minikubeCA.pem
I1128 20:02:46.360962    3242 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1128 20:02:46.374660    3242 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1128 20:02:46.384637    3242 kubeadm.go:390] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[Dashboard:kubernetesui/dashboard:v2.3.1@sha256:ec27f462cf1946220f5a9ace416a84a57c18f98c777876a8054405d1428cc92e IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 MetricsScraper:kubernetesui/metrics-scraper:v1.0.7@sha256:36d5b3f60e1a144cc5ada820910535074bdf5cf73fb70d1ff1681537eef4e172 MetricsServer:metrics-server/metrics-server:v0.4.2@sha256:dbc33d7d35d2a9cc5ab402005aa7a0d13be6192f3550c7d42cba8d2d5e3a5d62 Tiller:helm/tiller:v2.17.0@sha256:4c43eb385032945cad047d2350e4945d913b90b3ab43ee61cecb32a495c6df0f] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sal:/minikube-host}
I1128 20:02:46.384767    3242 ssh_runner.go:152] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1128 20:02:46.458741    3242 ssh_runner.go:152] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1128 20:02:46.468716    3242 kubeadm.go:401] found existing configuration files, will attempt cluster restart
I1128 20:02:46.468733    3242 kubeadm.go:600] restartCluster start
I1128 20:02:46.468766    3242 ssh_runner.go:152] Run: sudo test -d /data/minikube
I1128 20:02:46.482349    3242 kubeadm.go:126] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1128 20:02:46.488090    3242 kubeconfig.go:116] verify returned: extract IP: "minikube" does not appear in /home/sal/.kube/config
I1128 20:02:46.488207    3242 kubeconfig.go:127] "minikube" context is missing from /home/sal/.kube/config - will repair!
I1128 20:02:46.488755    3242 lock.go:35] WriteFile acquiring /home/sal/.kube/config: {Name:mk4a900981ed2079317c5cb8b170e54eadb3db4c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 20:02:46.539124    3242 ssh_runner.go:152] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1128 20:02:46.554177    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:46.554215    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:46.579451    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:46.779686    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:46.779768    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:46.795136    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:46.979984    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:46.980045    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:46.995449    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:47.180315    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:47.180376    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:47.195454    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:47.379775    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:47.379839    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:47.395116    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:47.579631    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:47.579759    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:47.594340    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:47.779804    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:47.779887    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:47.794449    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:47.979778    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:47.979901    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:47.997011    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:48.179928    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:48.179974    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:48.194319    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:48.380494    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:48.380682    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:48.395103    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:48.579720    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:48.579789    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:48.594899    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:48.781019    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:48.781083    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:48.797572    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:48.980781    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:48.980824    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:48.996966    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.180095    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:49.180152    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:49.195735    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.379782    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:49.380040    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:49.394771    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.580300    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:49.580375    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:49.594394    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.594401    3242 api_server.go:165] Checking apiserver status ...
I1128 20:02:49.594444    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 20:02:49.608412    3242 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.608422    3242 kubeadm.go:575] needs reconfigure: apiserver error: timed out waiting for the condition
I1128 20:02:49.608425    3242 kubeadm.go:1032] stopping kube-system containers ...
I1128 20:02:49.608496    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1128 20:02:49.659587    3242 docker.go:390] Stopping containers: [03bac0bf3d99 fb7c301b6c24 413886a92605 952a04b3687f 7dfc03499083 56a03b8cf0f2 e33cee2326a7 0e67252a7ec6 a4b2c4218de4 ea9e64ce2461 9a485f0799c1 d109410dd5dd 34bc47124f14 a33d61e38561 377d155d3515 d1e82442db5a f611d40a6786 8395f4fa3f7d 6342891a941e b7e2bbddc7a7 8c5b6fed0d4b d6fd52fa8826 be313ce360ed 47963099c307 f68f6c85937b dac2cbf3d4f8 01d24adcab26 0b255f63eb7e 305b398a1287]
I1128 20:02:49.659632    3242 ssh_runner.go:152] Run: docker stop 03bac0bf3d99 fb7c301b6c24 413886a92605 952a04b3687f 7dfc03499083 56a03b8cf0f2 e33cee2326a7 0e67252a7ec6 a4b2c4218de4 ea9e64ce2461 9a485f0799c1 d109410dd5dd 34bc47124f14 a33d61e38561 377d155d3515 d1e82442db5a f611d40a6786 8395f4fa3f7d 6342891a941e b7e2bbddc7a7 8c5b6fed0d4b d6fd52fa8826 be313ce360ed 47963099c307 f68f6c85937b dac2cbf3d4f8 01d24adcab26 0b255f63eb7e 305b398a1287
I1128 20:02:49.709991    3242 ssh_runner.go:152] Run: sudo systemctl stop kubelet
I1128 20:02:49.723211    3242 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1128 20:02:49.731678    3242 kubeadm.go:154] found existing configuration files:
-rw------- 1 root root 5643 Nov 25 21:57 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Nov 28 12:17 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Nov 25 21:57 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Nov 28 12:17 /etc/kubernetes/scheduler.conf

I1128 20:02:49.731734    3242 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1128 20:02:49.745652    3242 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1128 20:02:49.758948    3242 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1128 20:02:49.772844    3242 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.772877    3242 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1128 20:02:49.782388    3242 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1128 20:02:49.795027    3242 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1128 20:02:49.795062    3242 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1128 20:02:49.803263    3242 ssh_runner.go:152] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1128 20:02:49.812823    3242 kubeadm.go:676] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1128 20:02:49.812833    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1128 20:02:50.424504    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1128 20:02:51.104641    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1128 20:02:51.460493    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1128 20:02:51.548802    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1128 20:02:51.619967    3242 api_server.go:51] waiting for apiserver process to appear ...
I1128 20:02:51.620024    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:52.137977    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:52.638024    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:53.138287    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:53.638337    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:54.139143    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:54.638540    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:55.138144    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:55.637687    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:56.365678    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:56.639379    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:57.137930    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:02:57.258737    3242 api_server.go:71] duration metric: took 5.638770337s to wait for apiserver process to appear ...
I1128 20:02:57.258753    3242 api_server.go:87] waiting for apiserver healthz status ...
I1128 20:02:57.259603    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:02:57.265574    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:02:57.766686    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:02:57.767273    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:02:58.267246    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:02:58.267857    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:02:58.766180    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:02:58.766591    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:02:59.267070    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:02:59.267594    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:02:59.766143    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:02:59.766969    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:00.265971    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:00.266439    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:00.767650    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:00.768239    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:01.266287    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:01.266966    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:01.768054    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:01.768554    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:02.265845    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:02.266729    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:02.766181    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:02.766654    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:03.266233    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:03.268210    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:03.766552    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:03.767109    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:04.266194    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:04.266803    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:04.766111    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:04.766769    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:05.266322    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:05.266969    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:05.766315    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:05.767092    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:06.266754    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:06.267064    3242 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 20:03:06.766181    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:11.788226    3242 api_server.go:266] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1128 20:03:11.792027    3242 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1128 20:03:12.266636    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:12.360137    3242 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1128 20:03:12.360152    3242 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1128 20:03:12.765897    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:12.771544    3242 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1128 20:03:12.771558    3242 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1128 20:03:13.266728    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:13.278053    3242 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1128 20:03:13.278081    3242 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1128 20:03:13.767214    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:03:13.773754    3242 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I1128 20:03:13.882571    3242 api_server.go:140] control plane version: v1.22.3
I1128 20:03:13.882773    3242 api_server.go:130] duration metric: took 16.623942748s to wait for apiserver health ...
I1128 20:03:13.892449    3242 cni.go:93] Creating CNI manager for ""
I1128 20:03:13.892473    3242 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1128 20:03:13.892482    3242 system_pods.go:43] waiting for kube-system pods to appear ...
I1128 20:03:13.949575    3242 system_pods.go:59] 8 kube-system pods found
I1128 20:03:13.949600    3242 system_pods.go:61] "coredns-78fcd69978-dnt6d" [2ee5325d-9bb4-45d7-aab8-70db6e534d16] Running
I1128 20:03:13.949603    3242 system_pods.go:61] "etcd-minikube" [821cdc1c-ca8b-4d5c-9f40-3716a5d2446a] Running
I1128 20:03:13.949608    3242 system_pods.go:61] "kube-apiserver-minikube" [dbb7f681-a713-439f-9daf-72c99ae4e1a1] Running
I1128 20:03:13.949611    3242 system_pods.go:61] "kube-controller-manager-minikube" [45d3227f-706f-4bea-8c6f-c9f81e91d11d] Running
I1128 20:03:13.949613    3242 system_pods.go:61] "kube-proxy-t5sgt" [d5759552-13cb-451d-aa9e-b1ffdad1ec85] Running
I1128 20:03:13.949615    3242 system_pods.go:61] "kube-scheduler-minikube" [7579216a-38c3-4c25-94df-1dbb3b630200] Running
I1128 20:03:13.949618    3242 system_pods.go:61] "metrics-server-77c99ccb96-fmvvd" [5d4dd078-2eed-4ddb-abae-8b1a4e9b67b4] Running
I1128 20:03:13.949620    3242 system_pods.go:61] "storage-provisioner" [d9c9d770-cd9d-4a7c-accd-ed77131f86ea] Running
I1128 20:03:13.949623    3242 system_pods.go:74] duration metric: took 55.582359ms to wait for pod list to return data ...
I1128 20:03:13.950766    3242 node_conditions.go:102] verifying NodePressure condition ...
I1128 20:03:13.962932    3242 node_conditions.go:122] node storage ephemeral capacity is 205372392Ki
I1128 20:03:13.963076    3242 node_conditions.go:123] node cpu capacity is 4
I1128 20:03:13.963125    3242 node_conditions.go:105] duration metric: took 12.162459ms to run NodePressure ...
I1128 20:03:13.973177    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1128 20:03:15.267882    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.294676754s)
I1128 20:03:15.268186    3242 ssh_runner.go:152] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1128 20:03:15.387804    3242 ops.go:34] apiserver oom_adj: -16
I1128 20:03:15.388459    3242 kubeadm.go:604] restartCluster took 28.919078546s
I1128 20:03:15.388468    3242 kubeadm.go:392] StartCluster complete in 29.003840206s
I1128 20:03:15.409427    3242 settings.go:142] acquiring lock: {Name:mkc658d42049f234e0b058c49a3c8ebaecd489e3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 20:03:15.444053    3242 settings.go:150] Updating kubeconfig:  /home/sal/.kube/config
I1128 20:03:15.530904    3242 lock.go:35] WriteFile acquiring /home/sal/.kube/config: {Name:mk4a900981ed2079317c5cb8b170e54eadb3db4c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 20:03:15.584279    3242 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1128 20:03:15.600031    3242 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I1128 20:03:15.602477    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1128 20:03:15.616799    3242 start.go:229] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}
I1128 20:03:15.621823    3242 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1128 20:03:15.624395    3242 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1128 20:03:15.624407    3242 addons.go:65] Setting dashboard=true in profile "minikube"
I1128 20:03:15.624417    3242 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W1128 20:03:15.624422    3242 addons.go:165] addon storage-provisioner should already be in state true
I1128 20:03:15.624427    3242 addons.go:153] Setting addon dashboard=true in "minikube"
I1128 20:03:15.624429    3242 addons.go:65] Setting default-storageclass=true in profile "minikube"
W1128 20:03:15.624435    3242 addons.go:165] addon dashboard should already be in state true
I1128 20:03:15.624442    3242 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1128 20:03:15.637346    3242 host.go:66] Checking if "minikube" exists ...
I1128 20:03:15.637365    3242 host.go:66] Checking if "minikube" exists ...
I1128 20:03:15.651834    3242 addons.go:65] Setting metrics-server=true in profile "minikube"
I1128 20:03:15.651879    3242 addons.go:153] Setting addon metrics-server=true in "minikube"
W1128 20:03:15.651886    3242 addons.go:165] addon metrics-server should already be in state true
I1128 20:03:15.651994    3242 host.go:66] Checking if "minikube" exists ...
I1128 20:03:15.652905    3242 out.go:176] 🔎  Verifying Kubernetes components...
I1128 20:03:15.761487    3242 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I1128 20:03:15.770814    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1128 20:03:15.770841    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1128 20:03:15.770849    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1128 20:03:15.771647    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
W1128 20:04:04.442320    3242 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W1128 20:04:04.452915    3242 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W1128 20:04:04.466940    3242 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W1128 20:04:04.466961    3242 addons.go:202] "minikube" is not running, setting dashboard=true and skipping enablement (err=<nil>)
W1128 20:04:04.443745    3242 addons.go:202] "minikube" is not running, setting storage-provisioner=true and skipping enablement (err=<nil>)
W1128 20:04:04.517219    3242 addons_storage_classes.go:55] "minikube" is not running, writing default-storageclass=true to disk and skipping enablement
I1128 20:04:04.517270    3242 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1128 20:04:04.517283    3242 addons.go:165] addon default-storageclass should already be in state true
I1128 20:04:04.575018    3242 host.go:66] Checking if "minikube" exists ...
W1128 20:04:04.702206    3242 out.go:241] ❗  Executing "docker container inspect minikube --format={{.State.Status}}" took an unusually long time: 48.300399491s
W1128 20:04:04.703498    3242 out.go:241] 💡  Restarting the docker service may improve performance.
W1128 20:04:04.703558    3242 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W1128 20:04:04.703572    3242 addons.go:202] "minikube" is not running, setting metrics-server=true and skipping enablement (err=<nil>)
I1128 20:04:04.703580    3242 addons.go:386] Verifying addon metrics-server=true in "minikube"
I1128 20:04:04.709378    3242 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1128 20:04:07.247454    3242 cli_runner.go:168] Completed: docker container inspect minikube --format={{.State.Status}}: (2.532501609s)
I1128 20:04:07.401840    3242 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I1128 20:04:07.402738    3242 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1128 20:04:07.406014    3242 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 20:04:07.927033    3242 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/sal/.minikube/machines/minikube/id_rsa Username:docker}
I1128 20:05:05.305527    3242 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1128 20:05:05.749997    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1m50.147500826s)
I1128 20:05:05.828397    3242 ssh_runner.go:192] Completed: sudo systemctl is-active --quiet service kubelet: (1m49.98857671s)
I1128 20:05:05.946154    3242 start.go:719] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1128 20:05:06.125940    3242 api_server.go:51] waiting for apiserver process to appear ...
I1128 20:05:06.191850    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1128 20:05:10.732705    3242 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (5.427127536s)
I1128 20:05:10.777739    3242 out.go:176] 🌟  Complementos habilitados: storage-provisioner, dashboard, metrics-server, default-storageclass
I1128 20:05:10.778529    3242 addons.go:417] enableAddons completed in 1m55.16111401s
I1128 20:05:11.864173    3242 ssh_runner.go:192] Completed: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}: (5.672287742s)
I1128 20:05:11.864188    3242 logs.go:270] 2 containers: [6947bbacb1e6 7dfc03499083]
I1128 20:05:11.864227    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1128 20:05:12.182415    3242 logs.go:270] 2 containers: [355b40b6d663 34bc47124f14]
I1128 20:05:12.182508    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1128 20:05:12.427519    3242 logs.go:270] 2 containers: [1dc8c54d35fe e33cee2326a7]
I1128 20:05:12.428010    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1128 20:05:12.769079    3242 logs.go:270] 2 containers: [bfc517c58a4f 377d155d3515]
I1128 20:05:12.769139    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1128 20:05:13.075796    3242 logs.go:270] 2 containers: [889bed6b15fe ea9e64ce2461]
I1128 20:05:13.075861    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1128 20:05:13.353359    3242 logs.go:270] 2 containers: [e3165b893320 5c1aff244410]
I1128 20:05:13.353421    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1128 20:05:13.671110    3242 logs.go:270] 1 containers: [db7949701381]
I1128 20:05:13.671174    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1128 20:05:13.806373    3242 logs.go:270] 2 containers: [8c54193560a0 a33d61e38561]
I1128 20:05:13.806399    3242 logs.go:123] Gathering logs for dmesg ...
I1128 20:05:13.806407    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1128 20:05:14.005488    3242 logs.go:123] Gathering logs for kube-apiserver [7dfc03499083] ...
I1128 20:05:14.005503    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 7dfc03499083"
I1128 20:05:15.067816    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 7dfc03499083": (1.062294453s)
I1128 20:05:15.107202    3242 logs.go:123] Gathering logs for kube-scheduler [bfc517c58a4f] ...
I1128 20:05:15.107222    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 bfc517c58a4f"
I1128 20:05:15.282662    3242 logs.go:123] Gathering logs for kube-proxy [ea9e64ce2461] ...
I1128 20:05:15.282681    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 ea9e64ce2461"
I1128 20:05:15.881182    3242 logs.go:123] Gathering logs for kubernetes-dashboard [5c1aff244410] ...
I1128 20:05:15.881202    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 5c1aff244410"
I1128 20:05:16.045262    3242 logs.go:123] Gathering logs for Docker ...
I1128 20:05:16.045307    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1128 20:05:16.745185    3242 logs.go:123] Gathering logs for container status ...
I1128 20:05:16.764232    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1128 20:05:38.517328    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (21.752458055s)
I1128 20:05:38.526165    3242 logs.go:123] Gathering logs for kube-controller-manager [a33d61e38561] ...
I1128 20:05:38.527794    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 a33d61e38561"
I1128 20:05:40.162217    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 a33d61e38561": (1.634399351s)
I1128 20:05:40.225688    3242 logs.go:123] Gathering logs for kubelet ...
I1128 20:05:40.225742    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1128 20:05:40.621972    3242 logs.go:123] Gathering logs for etcd [355b40b6d663] ...
I1128 20:05:40.622003    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 355b40b6d663"
I1128 20:05:41.046428    3242 logs.go:123] Gathering logs for etcd [34bc47124f14] ...
I1128 20:05:41.046444    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 34bc47124f14"
I1128 20:05:41.647469    3242 logs.go:123] Gathering logs for coredns [e33cee2326a7] ...
I1128 20:05:41.647524    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 e33cee2326a7"
I1128 20:05:41.943842    3242 logs.go:123] Gathering logs for kube-scheduler [377d155d3515] ...
I1128 20:05:41.943862    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 377d155d3515"
I1128 20:05:42.288998    3242 logs.go:123] Gathering logs for kubernetes-dashboard [e3165b893320] ...
I1128 20:05:42.289079    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 e3165b893320"
I1128 20:05:42.575883    3242 logs.go:123] Gathering logs for storage-provisioner [db7949701381] ...
I1128 20:05:42.575900    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 db7949701381"
I1128 20:05:42.683607    3242 logs.go:123] Gathering logs for describe nodes ...
I1128 20:05:42.835160    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1128 20:06:14.422744    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (31.555891813s)
I1128 20:06:14.459582    3242 logs.go:123] Gathering logs for kube-apiserver [6947bbacb1e6] ...
I1128 20:06:14.478566    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 6947bbacb1e6"
I1128 20:06:20.311748    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 6947bbacb1e6": (5.829709941s)
I1128 20:06:20.369194    3242 logs.go:123] Gathering logs for coredns [1dc8c54d35fe] ...
I1128 20:06:20.372699    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 1dc8c54d35fe"
I1128 20:06:21.784410    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 1dc8c54d35fe": (1.408999408s)
I1128 20:06:21.786262    3242 logs.go:123] Gathering logs for kube-proxy [889bed6b15fe] ...
I1128 20:06:21.786346    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 889bed6b15fe"
I1128 20:06:22.284113    3242 logs.go:123] Gathering logs for kube-controller-manager [8c54193560a0] ...
I1128 20:06:22.284139    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 8c54193560a0"
I1128 20:06:25.366305    3242 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 20:06:25.769879    3242 api_server.go:71] duration metric: took 3m10.151346286s to wait for apiserver process to appear ...
I1128 20:06:25.776239    3242 api_server.go:87] waiting for apiserver healthz status ...
I1128 20:06:25.785785    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1128 20:06:26.242520    3242 logs.go:270] 2 containers: [6947bbacb1e6 7dfc03499083]
I1128 20:06:26.243275    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1128 20:06:26.303623    3242 logs.go:270] 2 containers: [355b40b6d663 34bc47124f14]
I1128 20:06:26.303703    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1128 20:06:26.355427    3242 logs.go:270] 2 containers: [1dc8c54d35fe e33cee2326a7]
I1128 20:06:26.355679    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1128 20:06:26.416238    3242 logs.go:270] 2 containers: [bfc517c58a4f 377d155d3515]
I1128 20:06:26.416304    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1128 20:06:26.486801    3242 logs.go:270] 2 containers: [889bed6b15fe ea9e64ce2461]
I1128 20:06:26.486852    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1128 20:06:26.756539    3242 logs.go:270] 2 containers: [e3165b893320 5c1aff244410]
I1128 20:06:26.757530    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1128 20:06:26.882582    3242 logs.go:270] 2 containers: [b9d773b2d0aa db7949701381]
I1128 20:06:26.882673    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1128 20:06:26.994390    3242 logs.go:270] 2 containers: [8c54193560a0 a33d61e38561]
I1128 20:06:26.996939    3242 logs.go:123] Gathering logs for kube-proxy [889bed6b15fe] ...
I1128 20:06:26.997535    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 889bed6b15fe"
I1128 20:06:27.060634    3242 logs.go:123] Gathering logs for kubernetes-dashboard [e3165b893320] ...
I1128 20:06:27.060665    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 e3165b893320"
I1128 20:06:27.112213    3242 logs.go:123] Gathering logs for kube-controller-manager [8c54193560a0] ...
I1128 20:06:27.112227    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 8c54193560a0"
I1128 20:06:27.192316    3242 logs.go:123] Gathering logs for etcd [34bc47124f14] ...
I1128 20:06:27.192333    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 34bc47124f14"
I1128 20:06:27.498821    3242 logs.go:123] Gathering logs for coredns [1dc8c54d35fe] ...
I1128 20:06:27.498852    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 1dc8c54d35fe"
I1128 20:06:27.568005    3242 logs.go:123] Gathering logs for coredns [e33cee2326a7] ...
I1128 20:06:27.568028    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 e33cee2326a7"
I1128 20:06:27.986648    3242 logs.go:123] Gathering logs for kube-scheduler [bfc517c58a4f] ...
I1128 20:06:27.986664    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 bfc517c58a4f"
I1128 20:06:28.178281    3242 logs.go:123] Gathering logs for kube-apiserver [7dfc03499083] ...
I1128 20:06:28.180410    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 7dfc03499083"
I1128 20:06:28.958129    3242 logs.go:123] Gathering logs for kube-scheduler [377d155d3515] ...
I1128 20:06:28.958151    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 377d155d3515"
I1128 20:06:53.872826    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 377d155d3515": (24.914655767s)
I1128 20:06:53.916618    3242 logs.go:123] Gathering logs for storage-provisioner [b9d773b2d0aa] ...
I1128 20:06:53.916636    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 b9d773b2d0aa"
I1128 20:06:55.761260    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 b9d773b2d0aa": (1.844581458s)
I1128 20:06:55.779552    3242 logs.go:123] Gathering logs for kube-controller-manager [a33d61e38561] ...
I1128 20:06:55.779569    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 a33d61e38561"
I1128 20:06:57.556590    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 a33d61e38561": (1.777001899s)
I1128 20:06:57.604404    3242 logs.go:123] Gathering logs for dmesg ...
I1128 20:06:57.605537    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1128 20:06:58.206246    3242 logs.go:123] Gathering logs for describe nodes ...
I1128 20:06:58.206703    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1128 20:07:59.553290    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (1m1.324890774s)
I1128 20:07:59.588844    3242 logs.go:123] Gathering logs for kube-apiserver [6947bbacb1e6] ...
I1128 20:07:59.595652    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 6947bbacb1e6"
I1128 20:08:26.262064    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 6947bbacb1e6": (26.663305842s)
I1128 20:08:26.314981    3242 logs.go:123] Gathering logs for kube-proxy [ea9e64ce2461] ...
I1128 20:08:26.321955    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 ea9e64ce2461"
I1128 20:08:28.812803    3242 ssh_runner.go:192] Completed: /bin/bash -c "docker logs --tail 400 ea9e64ce2461": (2.481337444s)
I1128 20:08:28.840515    3242 logs.go:123] Gathering logs for Docker ...
I1128 20:08:28.846937    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1128 20:08:30.251857    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo journalctl -u docker -n 400": (1.404615169s)
I1128 20:08:30.261322    3242 logs.go:123] Gathering logs for container status ...
I1128 20:08:30.264127    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1128 20:08:31.470907    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (1.206747069s)
I1128 20:08:31.483964    3242 logs.go:123] Gathering logs for kubelet ...
I1128 20:08:31.483982    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1128 20:08:32.524497    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo journalctl -u kubelet -n 400": (1.040326532s)
I1128 20:08:32.634888    3242 logs.go:123] Gathering logs for etcd [355b40b6d663] ...
I1128 20:08:32.634910    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 355b40b6d663"
I1128 20:08:33.427371    3242 logs.go:123] Gathering logs for kubernetes-dashboard [5c1aff244410] ...
I1128 20:08:33.427399    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 5c1aff244410"
I1128 20:08:34.295123    3242 logs.go:123] Gathering logs for storage-provisioner [db7949701381] ...
I1128 20:08:34.295143    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 db7949701381"
W1128 20:08:34.498605    3242 logs.go:130] failed storage-provisioner [db7949701381]: command: /bin/bash -c "docker logs --tail 400 db7949701381" /bin/bash -c "docker logs --tail 400 db7949701381": Process exited with status 1
stdout:

stderr:
Error: No such container: db7949701381
 output: 
** stderr ** 
Error: No such container: db7949701381

** /stderr **
I1128 20:08:37.017084    3242 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1128 20:08:37.490350    3242 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I1128 20:08:37.901411    3242 api_server.go:140] control plane version: v1.22.3
I1128 20:08:37.901439    3242 api_server.go:130] duration metric: took 2m12.123509677s to wait for apiserver health ...
I1128 20:08:37.911716    3242 system_pods.go:43] waiting for kube-system pods to appear ...
I1128 20:08:37.925109    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1128 20:08:38.211208    3242 logs.go:270] 2 containers: [6947bbacb1e6 7dfc03499083]
I1128 20:08:38.211440    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1128 20:08:38.337455    3242 logs.go:270] 2 containers: [355b40b6d663 34bc47124f14]
I1128 20:08:38.337685    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1128 20:08:38.439787    3242 logs.go:270] 2 containers: [1dc8c54d35fe e33cee2326a7]
I1128 20:08:38.439851    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1128 20:08:38.511092    3242 logs.go:270] 2 containers: [bfc517c58a4f 377d155d3515]
I1128 20:08:38.511226    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1128 20:08:38.596439    3242 logs.go:270] 2 containers: [889bed6b15fe ea9e64ce2461]
I1128 20:08:38.596522    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1128 20:08:38.681003    3242 logs.go:270] 2 containers: [e3165b893320 5c1aff244410]
I1128 20:08:38.681059    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1128 20:08:38.736370    3242 logs.go:270] 1 containers: [bd77db2fd8e4]
I1128 20:08:38.736429    3242 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1128 20:08:38.802680    3242 logs.go:270] 2 containers: [8c54193560a0 a33d61e38561]
I1128 20:08:38.803295    3242 logs.go:123] Gathering logs for etcd [355b40b6d663] ...
I1128 20:08:38.803662    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 355b40b6d663"
I1128 20:08:40.093669    3242 logs.go:123] Gathering logs for etcd [34bc47124f14] ...
I1128 20:08:40.093919    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 34bc47124f14"
I1128 20:08:40.560007    3242 logs.go:123] Gathering logs for kube-proxy [889bed6b15fe] ...
I1128 20:08:40.560027    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 889bed6b15fe"
I1128 20:08:40.661072    3242 logs.go:123] Gathering logs for kube-proxy [ea9e64ce2461] ...
I1128 20:08:40.661087    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 ea9e64ce2461"
I1128 20:08:40.763670    3242 logs.go:123] Gathering logs for Docker ...
I1128 20:08:40.763687    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1128 20:08:40.850821    3242 logs.go:123] Gathering logs for coredns [1dc8c54d35fe] ...
I1128 20:08:40.850839    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 1dc8c54d35fe"
I1128 20:08:40.940724    3242 logs.go:123] Gathering logs for coredns [e33cee2326a7] ...
I1128 20:08:40.940740    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 e33cee2326a7"
I1128 20:08:41.097033    3242 logs.go:123] Gathering logs for kube-scheduler [377d155d3515] ...
I1128 20:08:41.097052    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 377d155d3515"
I1128 20:08:41.243021    3242 logs.go:123] Gathering logs for kubernetes-dashboard [5c1aff244410] ...
I1128 20:08:41.243044    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 5c1aff244410"
I1128 20:08:41.365796    3242 logs.go:123] Gathering logs for dmesg ...
I1128 20:08:41.365814    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1128 20:08:41.397198    3242 logs.go:123] Gathering logs for describe nodes ...
I1128 20:08:41.397218    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1128 20:08:44.282398    3242 ssh_runner.go:192] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (2.884780181s)
I1128 20:08:44.290149    3242 logs.go:123] Gathering logs for kube-apiserver [6947bbacb1e6] ...
I1128 20:08:44.290170    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 6947bbacb1e6"
I1128 20:08:44.546248    3242 logs.go:123] Gathering logs for kube-scheduler [bfc517c58a4f] ...
I1128 20:08:44.546268    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 bfc517c58a4f"
I1128 20:08:44.606077    3242 logs.go:123] Gathering logs for kube-controller-manager [a33d61e38561] ...
I1128 20:08:44.606092    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 a33d61e38561"
I1128 20:08:44.886729    3242 logs.go:123] Gathering logs for kubelet ...
I1128 20:08:44.886749    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1128 20:08:45.012617    3242 logs.go:123] Gathering logs for kube-apiserver [7dfc03499083] ...
I1128 20:08:45.012634    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 7dfc03499083"
I1128 20:08:45.354032    3242 logs.go:123] Gathering logs for kubernetes-dashboard [e3165b893320] ...
I1128 20:08:45.354049    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 e3165b893320"
I1128 20:08:45.462271    3242 logs.go:123] Gathering logs for storage-provisioner [bd77db2fd8e4] ...
I1128 20:08:45.462288    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 bd77db2fd8e4"
I1128 20:08:45.577974    3242 logs.go:123] Gathering logs for kube-controller-manager [8c54193560a0] ...
I1128 20:08:45.577990    3242 ssh_runner.go:152] Run: /bin/bash -c "docker logs --tail 400 8c54193560a0"
I1128 20:08:45.768696    3242 logs.go:123] Gathering logs for container status ...
I1128 20:08:45.773449    3242 ssh_runner.go:152] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1128 20:08:49.318491    3242 system_pods.go:59] 8 kube-system pods found
I1128 20:08:49.319347    3242 system_pods.go:61] "coredns-78fcd69978-dnt6d" [2ee5325d-9bb4-45d7-aab8-70db6e534d16] Running
I1128 20:08:49.319355    3242 system_pods.go:61] "etcd-minikube" [821cdc1c-ca8b-4d5c-9f40-3716a5d2446a] Running
I1128 20:08:49.319358    3242 system_pods.go:61] "kube-apiserver-minikube" [dbb7f681-a713-439f-9daf-72c99ae4e1a1] Running
I1128 20:08:49.319361    3242 system_pods.go:61] "kube-controller-manager-minikube" [45d3227f-706f-4bea-8c6f-c9f81e91d11d] Running
I1128 20:08:49.319364    3242 system_pods.go:61] "kube-proxy-t5sgt" [d5759552-13cb-451d-aa9e-b1ffdad1ec85] Running
I1128 20:08:49.319367    3242 system_pods.go:61] "kube-scheduler-minikube" [7579216a-38c3-4c25-94df-1dbb3b630200] Running
I1128 20:08:49.319811    3242 system_pods.go:61] "metrics-server-77c99ccb96-fmvvd" [5d4dd078-2eed-4ddb-abae-8b1a4e9b67b4] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1128 20:08:49.319818    3242 system_pods.go:61] "storage-provisioner" [d9c9d770-cd9d-4a7c-accd-ed77131f86ea] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1128 20:08:49.319830    3242 system_pods.go:74] duration metric: took 11.408080868s to wait for pod list to return data ...
I1128 20:08:49.322135    3242 kubeadm.go:547] duration metric: took 5m33.703286197s to wait for : map[apiserver:true system_pods:true] ...
I1128 20:08:49.325224    3242 node_conditions.go:102] verifying NodePressure condition ...
I1128 20:08:49.609661    3242 node_conditions.go:122] node storage ephemeral capacity is 205372392Ki
I1128 20:08:49.613398    3242 node_conditions.go:123] node cpu capacity is 4
I1128 20:08:49.615139    3242 node_conditions.go:105] duration metric: took 288.81729ms to run NodePressure ...
I1128 20:08:49.618153    3242 start.go:234] waiting for startup goroutines ...
I1128 20:08:49.641098    3242 out.go:176] 💡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I1128 20:08:49.655534    3242 out.go:176] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Sun 2021-11-28 19:02:16 UTC, end at Sun 2021-11-28 19:17:44 UTC. --
Nov 28 19:02:16 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.400868288Z" level=info msg="Starting up"
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.420861525Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.420979722Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.421386102Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.421508506Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.432550462Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.432590180Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.432610564Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.432625849Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Nov 28 19:02:17 minikube dockerd[207]: time="2021-11-28T19:02:17.607383670Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.493591759Z" level=warning msg="Your kernel does not support swap memory limit"
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.493648175Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.493656976Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.493661589Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.496538383Z" level=info msg="Loading containers: start."
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.863667157Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Nov 28 19:02:18 minikube dockerd[207]: time="2021-11-28T19:02:18.953103911Z" level=info msg="Loading containers: done."
Nov 28 19:02:19 minikube dockerd[207]: time="2021-11-28T19:02:19.102567616Z" level=info msg="Docker daemon" commit=75249d8 graphdriver(s)=overlay2 version=20.10.8
Nov 28 19:02:19 minikube dockerd[207]: time="2021-11-28T19:02:19.111768717Z" level=info msg="Daemon has completed initialization"
Nov 28 19:02:19 minikube systemd[1]: Started Docker Application Container Engine.
Nov 28 19:02:19 minikube dockerd[207]: time="2021-11-28T19:02:19.163295588Z" level=info msg="API listen on [::]:2376"
Nov 28 19:02:19 minikube dockerd[207]: time="2021-11-28T19:02:19.169735273Z" level=info msg="API listen on /var/run/docker.sock"
Nov 28 19:04:04 minikube dockerd[207]: time="2021-11-28T19:04:04.683083715Z" level=info msg="ignoring event" container=db79497013816df5ac168614791d57dc08b672974e4d3bc7c7085f71238cec80 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:04:07 minikube dockerd[207]: time="2021-11-28T19:04:07.666946735Z" level=warning msg="Published ports are discarded when using host network mode"
Nov 28 19:04:07 minikube dockerd[207]: time="2021-11-28T19:04:07.817947879Z" level=warning msg="Published ports are discarded when using host network mode"
Nov 28 19:05:09 minikube dockerd[207]: time="2021-11-28T19:05:09.565395123Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Nov 28 19:05:09 minikube dockerd[207]: time="2021-11-28T19:05:09.876516092Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Nov 28 19:05:10 minikube dockerd[207]: time="2021-11-28T19:05:09.877854190Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Nov 28 19:05:12 minikube dockerd[207]: time="2021-11-28T19:05:12.861451827Z" level=info msg="ignoring event" container=0bebe81764d0168bb22f32e2a32ae64f1be6af9338fc42e2d74dab26b8ff2d31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:05:39 minikube dockerd[207]: time="2021-11-28T19:05:39.522379047Z" level=info msg="ignoring event" container=bd6d1ed695b8e3150dd514558f437dcfb6e1088357846280196d6d8415cec791 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:06:18 minikube dockerd[207]: time="2021-11-28T19:06:18.928048808Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Nov 28 19:06:22 minikube dockerd[207]: time="2021-11-28T19:06:22.053092489Z" level=info msg="ignoring event" container=856cebe0cc2b3030c5efca771084ad3c21e2f08db508ecc3ff97fb2c309e2237 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:06:58 minikube dockerd[207]: time="2021-11-28T19:06:58.235371724Z" level=info msg="ignoring event" container=b9d773b2d0aa742cc4777d07eebe87f1ccc4b95bccfd107f871d90dacb83924c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:07:06 minikube dockerd[207]: time="2021-11-28T19:07:06.990089098Z" level=info msg="ignoring event" container=4db05a37cae4104ab2a32c7c13548eaa148e9b337b7c402bc6cd6876e2599963 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:07:14 minikube dockerd[207]: time="2021-11-28T19:07:14.015868045Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Nov 28 19:07:54 minikube dockerd[207]: time="2021-11-28T19:07:54.089498720Z" level=info msg="ignoring event" container=e907ab5b56b547c43ad9f089c7e64372091d941049786b8bb8b7f63fe4b4e972 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:08:27 minikube dockerd[207]: time="2021-11-28T19:08:27.701081785Z" level=info msg="ignoring event" container=bd77db2fd8e4fed991859792661c14b5d8ae9d30ce52b764c96044280b825936 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:08:32 minikube dockerd[207]: time="2021-11-28T19:08:32.464131514Z" level=info msg="ignoring event" container=873ead20aea89579a600a3260c0423e5123474897a5077bccded3ae5250541ef module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:10:17 minikube dockerd[207]: time="2021-11-28T19:10:17.876933514Z" level=info msg="ignoring event" container=7721b86b0a3c2dfa1768f36bcf298db0579215de6deea62c59d209fc6fe3b647 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:12:01 minikube dockerd[207]: time="2021-11-28T19:12:01.051255094Z" level=info msg="ignoring event" container=9e8dd95b12670f1a2f1c01b85dfc5423a05729c7923604041e761a62c217cd72 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:12:08 minikube dockerd[207]: time="2021-11-28T19:12:08.669170785Z" level=info msg="ignoring event" container=a877a2facd69e1d9f4901934706aa44a2e348dd7dbb8d9761e900e7d6c9976e4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:12:08 minikube dockerd[207]: time="2021-11-28T19:12:08.967637632Z" level=info msg="ignoring event" container=1e3a6792a511576f6edafa1eb3cb3676e3b962b5bb2c442d8fb3bd4b47674fa3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:12:10 minikube dockerd[207]: time="2021-11-28T19:12:10.728283933Z" level=info msg="ignoring event" container=acc7bcf15874852caa247344a1542b62d5368bb5dab595c2facfbf9d404bfbcb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:12:11 minikube dockerd[207]: time="2021-11-28T19:12:11.006419221Z" level=info msg="ignoring event" container=4e5dbcdca24f810c57cc8a79f438a1b8cd5f2516b382fe766bb630117726d709 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:13:37 minikube dockerd[207]: time="2021-11-28T19:13:37.904921198Z" level=info msg="ignoring event" container=1857abbe33604ce9543a1d811b26578bb7b922afd6aa2f4f96328055fccfec1e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:13:38 minikube dockerd[207]: time="2021-11-28T19:13:38.307473126Z" level=info msg="ignoring event" container=2094dedf18350e5d1a112cbe98412a03333c267878dae3c85c3cd946824f82cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:14:24 minikube dockerd[207]: time="2021-11-28T19:14:24.555537587Z" level=info msg="ignoring event" container=6593f7d97c38b6bb2f119c667de8be3d0cb3e8ad6e98251f271c9699c722acd7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:14:24 minikube dockerd[207]: time="2021-11-28T19:14:24.700390105Z" level=info msg="ignoring event" container=5eb2223ce9e3d317c149790e27e4f5b58705eb361d156acc70e3d5810dc1af6f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 19:15:56 minikube dockerd[207]: time="2021-11-28T19:15:56.693943691Z" level=info msg="ignoring event" container=d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                            CREATED             STATE               NAME                        ATTEMPT             POD ID
d2a9807bffa51       6e38f40d628db                                                                    6 minutes ago       Exited              storage-provisioner         165                 b095e99b7226d
745d021f50b0f       17c225a562d97                                                                    7 minutes ago       Running             metrics-server              170                 29370a56fd9c2
873ead20aea89       17c225a562d97                                                                    9 minutes ago       Exited              metrics-server              169                 29370a56fd9c2
29ccdfb26da69       ubuntu@sha256:626ffe58f6e7566e00254b638eb7e0f3b11d4da9675088f4781a50ae288f3322   11 minutes ago      Running             ubuntu                      4                   57c03e89a3c0e
a92efac9ca787       86ea6f86fc575                                                                    12 minutes ago      Running             prometheus                  11                  040c07009c685
f18d019a564e6       7537d97cdf6f8                                                                    12 minutes ago      Running             cert-manager-webhook        84                  9aa7f5d033c8c
58ac741b67c51       2bf76755c714d                                                                    12 minutes ago      Running             kube-state-metrics          15                  2d8e989e65020
e3165b893320e       e1482a24335a6                                                                    12 minutes ago      Running             kubernetes-dashboard        47                  252a1acf9a9f1
0b6ed4f58ec5c       ab5b19f65726e                                                                    12 minutes ago      Running             cert-manager                75                  ad24f953c2bce
a4464354071dd       31880def345b4                                                                    12 minutes ago      Running             cainjector                  93                  e82cca4fe9aa6
268ee2d972b03       c19ae228f0699                                                                    12 minutes ago      Running             node-exporter               11                  3679b4f9cc34e
1dc8c54d35fe5       8d147537fb7d1                                                                    12 minutes ago      Running             coredns                     16                  2612232c7a0f8
da47f4d70baf3       7801cfc6d5c07                                                                    12 minutes ago      Running             dashboard-metrics-scraper   14                  6c4a9dd323494
cebb2a629a8da       def5ee20a468c                                                                    12 minutes ago      Running             kube-state-metrics          17                  31c598e36df61
0bebe81764d01       b0925e0819214                                                                    12 minutes ago      Exited              chown                       11                  040c07009c685
889bed6b15fee       6120bd723dced                                                                    14 minutes ago      Running             kube-proxy                  16                  9cb2ff3824211
bfc517c58a4fa       0aa9c7e31d307                                                                    14 minutes ago      Running             kube-scheduler              17                  e9eb0d59e744e
355b40b6d663f       0048118155842                                                                    14 minutes ago      Running             etcd                        17                  38bfc930d8516
6947bbacb1e6d       53224b502ea4d                                                                    14 minutes ago      Running             kube-apiserver              30                  e254e9e0338f9
8c54193560a00       05c905cef780c                                                                    14 minutes ago      Running             kube-controller-manager     18                  15ff0b692576d
7cc404da4017f       31880def345b4                                                                    5 hours ago         Exited              cainjector                  92                  f6d97383c86e5
5cb249f4c25af       ab5b19f65726e                                                                    5 hours ago         Exited              cert-manager                74                  7f7bef6c15f14
5c1aff244410d       e1482a24335a6                                                                    6 hours ago         Exited              kubernetes-dashboard        46                  4cb5226145c30
7dfc03499083c       53224b502ea4d                                                                    6 hours ago         Exited              kube-apiserver              29                  d1e82442db5a2
5f93573e1836f       7537d97cdf6f8                                                                    6 hours ago         Exited              cert-manager-webhook        83                  e9dc226c39909
cae9580d963c2       def5ee20a468c                                                                    7 hours ago         Exited              kube-state-metrics          16                  6798329a426ee
389fed7878686       ubuntu@sha256:626ffe58f6e7566e00254b638eb7e0f3b11d4da9675088f4781a50ae288f3322   7 hours ago         Exited              ubuntu                      3                   25183d81c9f52
d88eb7fdc25e6       86ea6f86fc575                                                                    7 hours ago         Exited              prometheus                  10                  e0f34841da493
e43a0eacd9ac5       c19ae228f0699                                                                    7 hours ago         Exited              node-exporter               10                  594ddb9f89f2a
2d9f9499766ab       2bf76755c714d                                                                    7 hours ago         Exited              kube-state-metrics          14                  d0e7967a1945f
fc1a90d103d42       7801cfc6d5c07                                                                    7 hours ago         Exited              dashboard-metrics-scraper   13                  90faad803aa6c
e33cee2326a7e       8d147537fb7d1                                                                    7 hours ago         Exited              coredns                     15                  a4b2c4218de49
ea9e64ce2461a       6120bd723dced                                                                    7 hours ago         Exited              kube-proxy                  15                  9a485f0799c11
34bc47124f14a       0048118155842                                                                    7 hours ago         Exited              etcd                        16                  8395f4fa3f7de
a33d61e385612       05c905cef780c                                                                    7 hours ago         Exited              kube-controller-manager     17                  6342891a941e1
377d155d35152       0aa9c7e31d307                                                                    7 hours ago         Exited              kube-scheduler              16                  f611d40a67864

* 
* ==> coredns [1dc8c54d35fe] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.674013894s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:51748->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: read udp 172.17.0.13:59917->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:58884->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:51468->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:44022->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:60770->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:55743->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: read udp 172.17.0.13:49366->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 kube-dns.kube-system.svc.cluster.local.localdomain. AAAA: read udp 172.17.0.13:40078->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 kube-dns.kube-system.svc.cluster.local.localdomain. A: read udp 172.17.0.13:49872->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:59022->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:45548->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:42612->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:33009->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:55617->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:47536->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:37514->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:57147->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: read udp 172.17.0.13:59527->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: read udp 172.17.0.13:59114->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.13:35653->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:55065->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 4.16582195s
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:35361->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.498528046s
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:41475->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.13:43721->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: read udp 172.17.0.13:53545->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: read udp 172.17.0.13:54148->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.377346693s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
W1128 19:10:17.604949       1 reflector.go:436] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1128 19:10:17.605076       1 reflector.go:436] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1128 19:10:17.604983       1 reflector.go:436] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding

* 
* ==> coredns [e33cee2326a7] <==
* [WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.7:52878->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.7:37887->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:48216->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:46585->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.900614482s
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:43867->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:52634->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:39146->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:57683->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:47957->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:57565->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:58297->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:58879->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. AAAA: read udp 172.17.0.7:52933->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:33663->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:54468->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:39747->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:38057->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:37755->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:52686->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.056393579s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.046624164s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.837386371s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.03805181s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:55760->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 app.netdata.cloud.localdomain. A: read udp 172.17.0.7:60983->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.158853562s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. A: dial udp 192.168.49.1:53: connect: network is unreachable
[ERROR] plugin/errors: 2 posthog.netdata.cloud.localdomain. AAAA: dial udp 192.168.49.1:53: connect: network is unreachable
E1128 13:43:19.888103       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
I1128 13:43:26.516518       1 trace.go:205] Trace[1117508154]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167 (28-Nov-2021 13:42:11.822) (total time: 74681ms):
Trace[1117508154]: [1m14.681344155s] [1m14.681344155s] END
E1128 13:43:26.535143       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:43:26.563772       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=80169": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:43:26.569509       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
I1128 13:43:53.666460       1 trace.go:205] Trace[676904208]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167 (28-Nov-2021 13:43:27.419) (total time: 26241ms):
Trace[676904208]: [26.241590428s] [26.241590428s] END
E1128 13:43:53.670919       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:43:53.694326       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:43:56.102180       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:43:56.399053       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=80169": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:44:03.902380       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:44:04.928562       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:44:14.998140       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=80169": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:44:15.151523       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:45:16.377973       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=80169": dial tcp 10.96.0.1:443: connect: connection refused
E1128 13:45:16.380142       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused
I1128 13:45:16.446701       1 trace.go:205] Trace[1126354657]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167 (28-Nov-2021 13:44:24.251) (total time: 51988ms):
Trace[1126354657]: [51.988793376s] [51.988793376s] END
E1128 13:45:16.457310       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=79515": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2021_11_25T22_57_32_0700
                    minikube.k8s.io/version=v1.24.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 25 Nov 2021 21:57:27 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Nov 2021 19:17:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Nov 2021 19:13:31 +0000   Sun, 28 Nov 2021 13:59:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Nov 2021 19:13:31 +0000   Sun, 28 Nov 2021 13:59:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Nov 2021 19:13:31 +0000   Sun, 28 Nov 2021 13:59:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Nov 2021 19:13:31 +0000   Sun, 28 Nov 2021 13:59:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  205372392Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3035488Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  205372392Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3035488Ki
  pods:               110
System Info:
  Machine ID:                 bba0be70c47c400ea3cf7733f1c0b4c1
  System UUID:                fc3fb999-c5bf-4382-9b17-9484476d2898
  Boot ID:                    94ce8795-9976-4ec9-a5b3-c32681f51875
  Kernel Version:             5.4.0-90-generic
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                   ------------  ----------  ---------------  -------------  ---
  default                     cert-manager-1638015062-cainjector-8658ffc775-cw2ng    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31h
  default                     cert-manager-1638015062-controller-5d54797847-w8jnf    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31h
  default                     cert-manager-1638015062-webhook-7ccbf994c-v6fbb        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31h
  default                     kube-state-metrics-1638013120-67d55bd875-kkq9r         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31h
  default                     sal-ubuntu                                             500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (4%!)(MISSING)       128Mi (4%!)(MISSING)     19h
  kube-system                 coredns-78fcd69978-dnt6d                               100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (2%!)(MISSING)        170Mi (5%!)(MISSING)     2d21h
  kube-system                 etcd-minikube                                          100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (3%!)(MISSING)       0 (0%!)(MISSING)         2d21h
  kube-system                 kube-apiserver-minikube                                250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d21h
  kube-system                 kube-controller-manager-minikube                       200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d21h
  kube-system                 kube-proxy-t5sgt                                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d21h
  kube-system                 kube-scheduler-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d21h
  kube-system                 metrics-server-77c99ccb96-fmvvd                        100m (2%!)(MISSING)     0 (0%!)(MISSING)      300Mi (10%!)(MISSING)      0 (0%!)(MISSING)         2d10h
  kube-system                 storage-provisioner                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d21h
  kubernetes-dashboard        dashboard-metrics-scraper-5594458c94-tmh5q             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d10h
  kubernetes-dashboard        kubernetes-dashboard-654cf69797-kv8sm                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d10h
  lens-metrics                kube-state-metrics-78596b555-7wt4g                     10m (0%!)(MISSING)      200m (5%!)(MISSING)   32Mi (1%!)(MISSING)        150Mi (5%!)(MISSING)     27h
  lens-metrics                node-exporter-svv24                                    10m (0%!)(MISSING)      200m (5%!)(MISSING)   24Mi (0%!)(MISSING)        100Mi (3%!)(MISSING)     27h
  lens-metrics                prometheus-0                                           100m (2%!)(MISSING)     0 (0%!)(MISSING)      512Mi (17%!)(MISSING)      0 (0%!)(MISSING)         27h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1470m (36%!)(MISSING)   900m (22%!)(MISSING)
  memory             1166Mi (39%!)(MISSING)  548Mi (18%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                  From        Message
  ----     ------                   ----                 ----        -------
  Normal   Starting                 12m                  kube-proxy  
  Warning  SystemOOM                5h38m                kubelet     System OOM encountered, victim process: cainjector, pid: 70062
  Warning  SystemOOM                5h38m                kubelet     System OOM encountered, victim process: netdata, pid: 8804
  Warning  SystemOOM                5h38m                kubelet     System OOM encountered, victim process: netdata, pid: 68518
  Normal   NodeHasSufficientMemory  5h17m (x39 over 7h)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    5h17m (x39 over 7h)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     5h17m (x39 over 7h)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                 14m                  kubelet     Starting kubelet.
  Normal   NodeHasSufficientMemory  14m (x8 over 14m)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    14m (x8 over 14m)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     14m (x7 over 14m)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  14m                  kubelet     Updated Node Allocatable limit across pods

* 
* ==> dmesg <==
* [  +0.108109] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12271 TOS=0x00 PREC=0x00 TTL=128 ID=41919 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +14.287854] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10948 TOS=0x00 PREC=0x00 TTL=128 ID=41987 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:02] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10948 TOS=0x00 PREC=0x00 TTL=128 ID=42272 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.750157] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12271 TOS=0x00 PREC=0x00 TTL=128 ID=42435 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +28.626758] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=43050 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.108026] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10975 TOS=0x00 PREC=0x00 TTL=128 ID=43051 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.110148] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10975 TOS=0x00 PREC=0x00 TTL=128 ID=43059 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.111912] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10975 TOS=0x00 PREC=0x00 TTL=128 ID=43067 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +1.005567] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10975 TOS=0x00 PREC=0x00 TTL=128 ID=43075 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +2.011701] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10975 TOS=0x00 PREC=0x00 TTL=128 ID=43083 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.032045] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12271 TOS=0x00 PREC=0x00 TTL=128 ID=43091 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.505115] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=43100 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +0.112494] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12298 TOS=0x00 PREC=0x00 TTL=128 ID=43101 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +7.254158] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12298 TOS=0x00 PREC=0x00 TTL=128 ID=43161 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:03] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10975 TOS=0x00 PREC=0x00 TTL=128 ID=43266 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +29.342060] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=43321 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +10.378774] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12325 TOS=0x00 PREC=0x00 TTL=128 ID=43429 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:04] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11002 TOS=0x00 PREC=0x00 TTL=128 ID=43468 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +10.311212] hrtimer: interrupt took 4952025 ns
[ +19.043446] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=43527 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +9.669845] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12325 TOS=0x00 PREC=0x00 TTL=128 ID=43580 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:05] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11029 TOS=0x00 PREC=0x00 TTL=128 ID=43675 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +29.361077] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=43883 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +12.823402] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12352 TOS=0x00 PREC=0x00 TTL=128 ID=43973 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:06] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12379 TOS=0x00 PREC=0x00 TTL=128 ID=44061 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +32.089038] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=44169 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +15.358485] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11083 TOS=0x00 PREC=0x00 TTL=128 ID=44223 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:07] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11083 TOS=0x00 PREC=0x00 TTL=128 ID=44332 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +16.436406] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12406 TOS=0x00 PREC=0x00 TTL=128 ID=44479 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +20.233801] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11110 TOS=0x00 PREC=0x00 TTL=128 ID=44567 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:08] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12433 TOS=0x00 PREC=0x00 TTL=128 ID=44707 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +23.871749] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12433 TOS=0x00 PREC=0x00 TTL=128 ID=44824 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +17.065391] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11137 TOS=0x00 PREC=0x00 TTL=128 ID=44885 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:09] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12460 TOS=0x00 PREC=0x00 TTL=128 ID=44964 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +24.009916] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12460 TOS=0x00 PREC=0x00 TTL=128 ID=44996 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +13.920716] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11164 TOS=0x00 PREC=0x00 TTL=128 ID=45054 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:10] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=45082 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +31.368828] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12487 TOS=0x00 PREC=0x00 TTL=128 ID=45169 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +10.779877] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11191 TOS=0x00 PREC=0x00 TTL=128 ID=45219 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:11] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11720 TOS=0x00 PREC=0x00 TTL=128 ID=45248 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK URGP=0 
[ +31.872826] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12514 TOS=0x00 PREC=0x00 TTL=128 ID=45338 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[  +7.621846] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11218 TOS=0x00 PREC=0x00 TTL=128 ID=45390 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:12] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=10260 TOS=0x00 PREC=0x00 TTL=128 ID=45422 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK URGP=0 
[ +16.216494] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12541 TOS=0x00 PREC=0x00 TTL=128 ID=45499 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +20.574851] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11245 TOS=0x00 PREC=0x00 TTL=128 ID=45561 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:13] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11245 TOS=0x00 PREC=0x00 TTL=128 ID=45577 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +19.366052] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12568 TOS=0x00 PREC=0x00 TTL=128 ID=45655 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +13.320720] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11272 TOS=0x00 PREC=0x00 TTL=128 ID=45698 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:14] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11272 TOS=0x00 PREC=0x00 TTL=128 ID=45733 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +14.502293] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12595 TOS=0x00 PREC=0x00 TTL=128 ID=45796 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +18.211875] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11299 TOS=0x00 PREC=0x00 TTL=128 ID=45852 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:15] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11299 TOS=0x00 PREC=0x00 TTL=128 ID=45885 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +10.639074] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12622 TOS=0x00 PREC=0x00 TTL=128 ID=45921 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +20.057205] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11326 TOS=0x00 PREC=0x00 TTL=128 ID=46040 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:16] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11326 TOS=0x00 PREC=0x00 TTL=128 ID=46082 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +12.941862] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=12622 TOS=0x00 PREC=0x00 TTL=128 ID=46091 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +17.736932] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11353 TOS=0x00 PREC=0x00 TTL=128 ID=46193 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[Nov28 19:17] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11353 TOS=0x00 PREC=0x00 TTL=128 ID=46239 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 
[ +16.085286] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=11720 TOS=0x00 PREC=0x00 TTL=128 ID=46249 PROTO=TCP SPT=443 DPT=40400 WINDOW=64240 RES=0x00 ACK URGP=0 
[ +13.279809] [UFW BLOCK] IN=ens32 OUT= MAC=00:0c:29:b5:02:66:00:50:56:f9:3b:09:08:00 SRC=134.122.94.167 DST=192.168.100.129 LEN=67 TOS=0x00 PREC=0x00 TTL=128 ID=46316 PROTO=TCP SPT=443 DPT=50208 WINDOW=64240 RES=0x00 ACK PSH URGP=0 

* 
* ==> etcd [34bc47124f14] <==
* {"level":"info","ts":"2021-11-28T13:59:48.270Z","caller":"traceutil/trace.go:171","msg":"trace[1405194220] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:81027; }","duration":"181.387932ms","start":"2021-11-28T13:59:48.088Z","end":"2021-11-28T13:59:48.270Z","steps":["trace[1405194220] 'agreement among raft nodes before linearized reading'  (duration: 179.637589ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T13:59:48.273Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.53518ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" ","response":"range_response_count:1 size:255"}
{"level":"info","ts":"2021-11-28T13:59:48.273Z","caller":"traceutil/trace.go:171","msg":"trace[373703459] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:81027; }","duration":"101.694596ms","start":"2021-11-28T13:59:48.171Z","end":"2021-11-28T13:59:48.273Z","steps":["trace[373703459] 'agreement among raft nodes before linearized reading'  (duration: 99.809948ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T13:59:49.903Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.828457ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" ","response":"range_response_count:12 size:13526"}
{"level":"info","ts":"2021-11-28T13:59:49.903Z","caller":"traceutil/trace.go:171","msg":"trace[1133286142] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:12; response_revision:81041; }","duration":"107.037768ms","start":"2021-11-28T13:59:49.796Z","end":"2021-11-28T13:59:49.903Z","steps":["trace[1133286142] 'range keys from in-memory index tree'  (duration: 61.939889ms)","trace[1133286142] 'range keys from bolt db'  (duration: 32.74216ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T13:59:50.987Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"211.622296ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" ","response":"range_response_count:21 size:103733"}
{"level":"info","ts":"2021-11-28T13:59:50.987Z","caller":"traceutil/trace.go:171","msg":"trace[464412227] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:21; response_revision:81058; }","duration":"211.709529ms","start":"2021-11-28T13:59:50.775Z","end":"2021-11-28T13:59:50.987Z","steps":["trace[464412227] 'agreement among raft nodes before linearized reading'  (duration: 14.399142ms)","trace[464412227] 'range keys from in-memory index tree'  (duration: 22.211286ms)","trace[464412227] 'range keys from bolt db'  (duration: 174.941977ms)"],"step_count":3}
{"level":"warn","ts":"2021-11-28T13:59:51.791Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"104.966316ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009306599455825 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/coredns-78fcd69978-dnt6d.16bbb5af84ed7b4d\" mod_revision:80355 > success:<request_put:<key:\"/registry/events/kube-system/coredns-78fcd69978-dnt6d.16bbb5af84ed7b4d\" value_size:685 lease:8128009306599455655 >> failure:<request_range:<key:\"/registry/events/kube-system/coredns-78fcd69978-dnt6d.16bbb5af84ed7b4d\" > >>","response":"size:18"}
{"level":"info","ts":"2021-11-28T13:59:51.793Z","caller":"traceutil/trace.go:171","msg":"trace[1651565920] transaction","detail":"{read_only:false; response_revision:81077; number_of_response:1; }","duration":"177.887174ms","start":"2021-11-28T13:59:51.615Z","end":"2021-11-28T13:59:51.793Z","steps":["trace[1651565920] 'process raft request'  (duration: 70.968876ms)"],"step_count":1}
{"level":"info","ts":"2021-11-28T13:59:51.793Z","caller":"traceutil/trace.go:171","msg":"trace[1041604550] linearizableReadLoop","detail":"{readStateIndex:97488; appliedIndex:97485; }","duration":"174.265383ms","start":"2021-11-28T13:59:51.619Z","end":"2021-11-28T13:59:51.793Z","steps":["trace[1041604550] 'read index received'  (duration: 259.402µs)","trace[1041604550] 'applied index is now lower than readState.Index'  (duration: 174.001479ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T13:59:51.815Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"195.580495ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/daemon-set-controller\" ","response":"range_response_count:1 size:261"}
{"level":"info","ts":"2021-11-28T13:59:51.815Z","caller":"traceutil/trace.go:171","msg":"trace[1613708030] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/daemon-set-controller; range_end:; response_count:1; response_revision:81077; }","duration":"195.729227ms","start":"2021-11-28T13:59:51.619Z","end":"2021-11-28T13:59:51.815Z","steps":["trace[1613708030] 'agreement among raft nodes before linearized reading'  (duration: 174.453081ms)","trace[1613708030] 'range keys from in-memory index tree'  (duration: 17.7588ms)"],"step_count":2}
{"level":"info","ts":"2021-11-28T13:59:51.880Z","caller":"traceutil/trace.go:171","msg":"trace[2122501992] transaction","detail":"{read_only:false; response_revision:81078; number_of_response:1; }","duration":"202.48632ms","start":"2021-11-28T13:59:51.677Z","end":"2021-11-28T13:59:51.880Z","steps":["trace[2122501992] 'process raft request'  (duration: 123.205784ms)","trace[2122501992] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/pods/default/cert-manager-1638015062-controller-5d54797847-w8jnf; req_size:4324; } (duration: 63.605768ms)"],"step_count":2}
{"level":"info","ts":"2021-11-28T13:59:51.895Z","caller":"traceutil/trace.go:171","msg":"trace[330822043] linearizableReadLoop","detail":"{readStateIndex:97490; appliedIndex:97488; }","duration":"102.046419ms","start":"2021-11-28T13:59:51.793Z","end":"2021-11-28T13:59:51.895Z","steps":["trace[330822043] 'read index received'  (duration: 12.25095ms)","trace[330822043] 'applied index is now lower than readState.Index'  (duration: 89.793089ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T13:59:51.912Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"238.859894ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T13:59:51.912Z","caller":"traceutil/trace.go:171","msg":"trace[711409435] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:81079; }","duration":"238.947101ms","start":"2021-11-28T13:59:51.673Z","end":"2021-11-28T13:59:51.912Z","steps":["trace[711409435] 'agreement among raft nodes before linearized reading'  (duration: 223.189397ms)"],"step_count":1}
{"level":"info","ts":"2021-11-28T13:59:51.912Z","caller":"traceutil/trace.go:171","msg":"trace[708799838] transaction","detail":"{read_only:false; response_revision:81079; number_of_response:1; }","duration":"224.597068ms","start":"2021-11-28T13:59:51.687Z","end":"2021-11-28T13:59:51.912Z","steps":["trace[708799838] 'process raft request'  (duration: 189.034221ms)","trace[708799838] 'compare'  (duration: 13.858976ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T13:59:51.912Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"152.176963ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/system-nodes\" ","response":"range_response_count:1 size:1082"}
{"level":"info","ts":"2021-11-28T13:59:51.912Z","caller":"traceutil/trace.go:171","msg":"trace[1595350674] range","detail":"{range_begin:/registry/flowschemas/system-nodes; range_end:; response_count:1; response_revision:81079; }","duration":"152.205297ms","start":"2021-11-28T13:59:51.760Z","end":"2021-11-28T13:59:51.912Z","steps":["trace[1595350674] 'agreement among raft nodes before linearized reading'  (duration: 152.151461ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T13:59:51.924Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"253.778935ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-c6d3377c-1225-431e-b265-9155ce4bbf28\" ","response":"range_response_count:1 size:1133"}
{"level":"info","ts":"2021-11-28T13:59:51.925Z","caller":"traceutil/trace.go:171","msg":"trace[1474913086] range","detail":"{range_begin:/registry/persistentvolumes/pvc-c6d3377c-1225-431e-b265-9155ce4bbf28; range_end:; response_count:1; response_revision:81079; }","duration":"254.024476ms","start":"2021-11-28T13:59:51.670Z","end":"2021-11-28T13:59:51.924Z","steps":["trace[1474913086] 'agreement among raft nodes before linearized reading'  (duration: 225.423114ms)","trace[1474913086] 'range keys from in-memory index tree'  (duration: 27.625059ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T14:00:33.245Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.153133622s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T14:00:33.257Z","caller":"traceutil/trace.go:171","msg":"trace[863498827] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:81231; }","duration":"1.180416701s","start":"2021-11-28T14:00:32.076Z","end":"2021-11-28T14:00:33.256Z","steps":["trace[863498827] 'range keys from in-memory index tree'  (duration: 1.152042197s)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:00:33.282Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.191062501s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" ","response":"range_response_count:21 size:102344"}
{"level":"info","ts":"2021-11-28T14:00:33.282Z","caller":"traceutil/trace.go:171","msg":"trace[496942489] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:21; response_revision:81231; }","duration":"1.191148504s","start":"2021-11-28T14:00:32.091Z","end":"2021-11-28T14:00:33.282Z","steps":["trace[496942489] 'range keys from bolt db'  (duration: 1.19085862s)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:00:33.625Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:00:32.076Z","time spent":"1.181508021s","remote":"127.0.0.1:55152","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2021-11-28T14:00:33.626Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:00:32.091Z","time spent":"1.191194093s","remote":"127.0.0.1:55350","response type":"/etcdserverpb.KV/Range","request count":0,"request size":34,"response count":21,"response size":102368,"request content":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" "}
{"level":"warn","ts":"2021-11-28T14:00:34.027Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"386.721902ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009306599456204 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:81230 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:474 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2021-11-28T14:00:34.027Z","caller":"traceutil/trace.go:171","msg":"trace[1273253255] transaction","detail":"{read_only:false; response_revision:81232; number_of_response:1; }","duration":"393.304438ms","start":"2021-11-28T14:00:33.634Z","end":"2021-11-28T14:00:34.027Z","steps":["trace[1273253255] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-node-lease/minikube; req_size:520; } (duration: 386.61112ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:00:34.028Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:00:33.634Z","time spent":"393.609583ms","remote":"127.0.0.1:33872","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":523,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:81230 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:474 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2021-11-28T14:00:34.028Z","caller":"traceutil/trace.go:171","msg":"trace[434003551] linearizableReadLoop","detail":"{readStateIndex:97651; appliedIndex:97650; }","duration":"392.490029ms","start":"2021-11-28T14:00:33.635Z","end":"2021-11-28T14:00:34.027Z","steps":["trace[434003551] 'read index received'  (duration: 2.711131ms)","trace[434003551] 'applied index is now lower than readState.Index'  (duration: 389.777694ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T14:00:34.029Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"391.222077ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-b4ae85de-7238-462f-a9ca-7f3974636e0b\" ","response":"range_response_count:1 size:1132"}
{"level":"info","ts":"2021-11-28T14:00:34.029Z","caller":"traceutil/trace.go:171","msg":"trace[1264608367] range","detail":"{range_begin:/registry/persistentvolumes/pvc-b4ae85de-7238-462f-a9ca-7f3974636e0b; range_end:; response_count:1; response_revision:81232; }","duration":"391.322411ms","start":"2021-11-28T14:00:33.638Z","end":"2021-11-28T14:00:34.029Z","steps":["trace[1264608367] 'agreement among raft nodes before linearized reading'  (duration: 390.852526ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:00:34.030Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:00:33.638Z","time spent":"391.750624ms","remote":"127.0.0.1:55288","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1156,"request content":"key:\"/registry/persistentvolumes/pvc-b4ae85de-7238-462f-a9ca-7f3974636e0b\" "}
{"level":"warn","ts":"2021-11-28T14:00:34.030Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"395.169666ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c\" ","response":"range_response_count:1 size:1136"}
{"level":"info","ts":"2021-11-28T14:00:34.030Z","caller":"traceutil/trace.go:171","msg":"trace[1344431437] range","detail":"{range_begin:/registry/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c; range_end:; response_count:1; response_revision:81232; }","duration":"395.224874ms","start":"2021-11-28T14:00:33.635Z","end":"2021-11-28T14:00:34.030Z","steps":["trace[1344431437] 'agreement among raft nodes before linearized reading'  (duration: 393.869271ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:00:34.030Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:00:33.635Z","time spent":"395.262012ms","remote":"127.0.0.1:55288","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1160,"request content":"key:\"/registry/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c\" "}
{"level":"info","ts":"2021-11-28T14:00:50.888Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":80939}
{"level":"warn","ts":"2021-11-28T14:00:57.287Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"727.302365ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:113008"}
{"level":"info","ts":"2021-11-28T14:00:57.288Z","caller":"traceutil/trace.go:171","msg":"trace[1357381921] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:81234; }","duration":"728.046705ms","start":"2021-11-28T14:00:56.559Z","end":"2021-11-28T14:00:57.287Z","steps":["trace[1357381921] 'range keys from bolt db'  (duration: 727.180035ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:00:57.290Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:00:56.559Z","time spent":"730.81293ms","remote":"127.0.0.1:55362","response type":"/etcdserverpb.KV/Range","request count":0,"request size":29,"response count":1,"response size":113032,"request content":"key:\"/registry/ranges/serviceips\" "}
{"level":"info","ts":"2021-11-28T14:00:57.995Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":80939,"took":"7.10455342s"}
{"level":"warn","ts":"2021-11-28T14:00:58.015Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"102.218961ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009306599456239 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:80914 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1024 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2021-11-28T14:00:58.016Z","caller":"traceutil/trace.go:171","msg":"trace[1208913736] transaction","detail":"{read_only:false; response_revision:81235; number_of_response:1; }","duration":"103.997965ms","start":"2021-11-28T14:00:57.911Z","end":"2021-11-28T14:00:58.015Z","steps":["trace[1208913736] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1094; } (duration: 101.79302ms)"],"step_count":1}
{"level":"info","ts":"2021-11-28T14:00:58.017Z","caller":"traceutil/trace.go:171","msg":"trace[702651926] linearizableReadLoop","detail":"{readStateIndex:97655; appliedIndex:97654; }","duration":"103.420691ms","start":"2021-11-28T14:00:57.914Z","end":"2021-11-28T14:00:58.017Z","steps":["trace[702651926] 'read index received'  (duration: 65.71µs)","trace[702651926] 'applied index is now lower than readState.Index'  (duration: 103.352414ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T14:00:58.017Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.528386ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/monitoring.coreos.com/servicemonitors/\" range_end:\"/registry/monitoring.coreos.com/servicemonitors0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T14:00:58.022Z","caller":"traceutil/trace.go:171","msg":"trace[1464317502] range","detail":"{range_begin:/registry/monitoring.coreos.com/servicemonitors/; range_end:/registry/monitoring.coreos.com/servicemonitors0; response_count:0; response_revision:81235; }","duration":"108.097242ms","start":"2021-11-28T14:00:57.913Z","end":"2021-11-28T14:00:58.022Z","steps":["trace[1464317502] 'agreement among raft nodes before linearized reading'  (duration: 103.502499ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T14:02:23.576Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"371.190259ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009306599456757 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc7d667aedfff4>","response":"size:42"}
{"level":"warn","ts":"2021-11-28T14:02:23.582Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T14:02:23.194Z","time spent":"385.906398ms","remote":"127.0.0.1:55240","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2021-11-28T14:03:03.524Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"119.168031ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2021-11-28T14:03:03.557Z","caller":"traceutil/trace.go:171","msg":"trace[1080267097] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:81356; }","duration":"178.767916ms","start":"2021-11-28T14:03:03.372Z","end":"2021-11-28T14:03:03.551Z","steps":["trace[1080267097] 'range keys from in-memory index tree'  (duration: 118.962575ms)"],"step_count":1}
{"level":"info","ts":"2021-11-28T14:03:06.031Z","caller":"traceutil/trace.go:171","msg":"trace[953882981] transaction","detail":"{read_only:false; response_revision:81358; number_of_response:1; }","duration":"128.420394ms","start":"2021-11-28T14:03:05.896Z","end":"2021-11-28T14:03:06.024Z","steps":["trace[953882981] 'process raft request'  (duration: 95.829863ms)","trace[953882981] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/configmaps/kube-system/cert-manager-controller; req_size:607; } (duration: 30.376077ms)"],"step_count":2}
{"level":"info","ts":"2021-11-28T14:03:06.265Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2021-11-28T14:03:06.365Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2021/11/28 14:03:07 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2021/11/28 14:03:07 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2021-11-28T14:03:07.831Z","caller":"etcdserver/server.go:1438","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2021-11-28T14:03:07.870Z","caller":"embed/etcd.go:562","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2021-11-28T14:03:07.872Z","caller":"embed/etcd.go:567","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2021-11-28T14:03:07.872Z","caller":"embed/etcd.go:369","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [355b40b6d663] <==
* {"level":"warn","ts":"2021-11-28T19:16:14.126Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"563.416839ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder\" ","response":"range_response_count:1 size:3260"}
{"level":"warn","ts":"2021-11-28T19:16:14.127Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:16:12.704Z","time spent":"1.422094605s","remote":"127.0.0.1:45402","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":1,"response size":1106,"request content":"key:\"/registry/flowschemas/system-nodes\" "}
{"level":"info","ts":"2021-11-28T19:16:14.127Z","caller":"traceutil/trace.go:171","msg":"trace[2003111188] range","detail":"{range_begin:/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder; range_end:; response_count:1; response_revision:82358; }","duration":"564.20204ms","start":"2021-11-28T19:16:13.562Z","end":"2021-11-28T19:16:14.127Z","steps":["trace[2003111188] 'range keys from in-memory index tree'  (duration: 561.775015ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:16:14.126Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.42254211s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/monitoring.coreos.com/servicemonitors/\" range_end:\"/registry/monitoring.coreos.com/servicemonitors0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2021-11-28T19:16:14.128Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"132.206478ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009312824237877 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/configmaps/kube-system/cert-manager-cainjector-leader-election\" mod_revision:82350 > success:<request_put:<key:\"/registry/configmaps/kube-system/cert-manager-cainjector-leader-election\" value_size:564 >> failure:<request_range:<key:\"/registry/configmaps/kube-system/cert-manager-cainjector-leader-election\" > >>","response":"size:18"}
{"level":"info","ts":"2021-11-28T19:16:14.128Z","caller":"traceutil/trace.go:171","msg":"trace[1142178867] range","detail":"{range_begin:/registry/monitoring.coreos.com/servicemonitors/; range_end:/registry/monitoring.coreos.com/servicemonitors0; response_count:0; response_revision:82358; }","duration":"1.424923998s","start":"2021-11-28T19:16:12.703Z","end":"2021-11-28T19:16:14.128Z","steps":["trace[1142178867] 'count revisions from in-memory index tree'  (duration: 1.42034723s)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:16:14.130Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:16:12.703Z","time spent":"1.426470442s","remote":"127.0.0.1:45430","response type":"/etcdserverpb.KV/Range","request count":0,"request size":102,"response count":0,"response size":30,"request content":"key:\"/registry/monitoring.coreos.com/servicemonitors/\" range_end:\"/registry/monitoring.coreos.com/servicemonitors0\" count_only:true "}
{"level":"info","ts":"2021-11-28T19:16:14.129Z","caller":"traceutil/trace.go:171","msg":"trace[858829837] transaction","detail":"{read_only:false; response_revision:82359; number_of_response:1; }","duration":"134.426776ms","start":"2021-11-28T19:16:13.994Z","end":"2021-11-28T19:16:14.129Z","steps":["trace[858829837] 'compare'  (duration: 128.731316ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:16:14.129Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:16:13.562Z","time spent":"564.775458ms","remote":"127.0.0.1:45416","response type":"/etcdserverpb.KV/Range","request count":0,"request size":72,"response count":1,"response size":3284,"request content":"key:\"/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder\" "}
{"level":"warn","ts":"2021-11-28T19:16:27.156Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"977.190644ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009312824237960 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc7d67edf49f87>","response":"size:42"}
{"level":"warn","ts":"2021-11-28T19:16:27.162Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:16:26.176Z","time spent":"986.066617ms","remote":"127.0.0.1:40646","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2021-11-28T19:16:27.162Z","caller":"traceutil/trace.go:171","msg":"trace[320329539] linearizableReadLoop","detail":"{readStateIndex:98979; appliedIndex:98978; }","duration":"754.35807ms","start":"2021-11-28T19:16:26.407Z","end":"2021-11-28T19:16:27.161Z","steps":["trace[320329539] 'read index received'  (duration: 21.061µs)","trace[320329539] 'applied index is now lower than readState.Index'  (duration: 754.334623ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:16:27.166Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"364.875805ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingressclasses/\" range_end:\"/registry/ingressclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T19:16:27.166Z","caller":"traceutil/trace.go:171","msg":"trace[394561365] range","detail":"{range_begin:/registry/ingressclasses/; range_end:/registry/ingressclasses0; response_count:0; response_revision:82365; }","duration":"364.943211ms","start":"2021-11-28T19:16:26.801Z","end":"2021-11-28T19:16:27.166Z","steps":["trace[394561365] 'agreement among raft nodes before linearized reading'  (duration: 364.853066ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:16:27.166Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:16:26.801Z","time spent":"365.004542ms","remote":"127.0.0.1:40716","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":0,"response size":30,"request content":"key:\"/registry/ingressclasses/\" range_end:\"/registry/ingressclasses0\" count_only:true "}
{"level":"warn","ts":"2021-11-28T19:16:27.166Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"757.15606ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T19:16:27.169Z","caller":"traceutil/trace.go:171","msg":"trace[1941522406] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:82365; }","duration":"759.949738ms","start":"2021-11-28T19:16:26.407Z","end":"2021-11-28T19:16:27.167Z","steps":["trace[1941522406] 'agreement among raft nodes before linearized reading'  (duration: 757.02649ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:16:27.170Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:16:26.407Z","time spent":"762.33246ms","remote":"127.0.0.1:40636","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2021-11-28T19:17:13.523Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"809.063224ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 ","response":"range_response_count:322 size:236200"}
{"level":"info","ts":"2021-11-28T19:17:13.543Z","caller":"traceutil/trace.go:171","msg":"trace[1598878782] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:322; response_revision:82387; }","duration":"840.114518ms","start":"2021-11-28T19:17:12.700Z","end":"2021-11-28T19:17:13.540Z","steps":["trace[1598878782] 'range keys from in-memory index tree'  (duration: 113.48519ms)","trace[1598878782] 'range keys from bolt db'  (duration: 690.870097ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:13.546Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:12.700Z","time spent":"845.157243ms","remote":"127.0.0.1:40650","response type":"/etcdserverpb.KV/Range","request count":0,"request size":41,"response count":322,"response size":236224,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 "}
{"level":"warn","ts":"2021-11-28T19:17:14.693Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"108.019673ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009312824238182 > lease_revoke:<id:70cc7d67edf4a005>","response":"size:30"}
{"level":"warn","ts":"2021-11-28T19:17:18.586Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"425.86445ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009312824238201 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/cert-manager-controller\" mod_revision:82385 > success:<request_put:<key:\"/registry/leases/kube-system/cert-manager-controller\" value_size:457 >> failure:<request_range:<key:\"/registry/leases/kube-system/cert-manager-controller\" > >>","response":"size:18"}
{"level":"info","ts":"2021-11-28T19:17:18.611Z","caller":"traceutil/trace.go:171","msg":"trace[1679139445] transaction","detail":"{read_only:false; response_revision:82389; number_of_response:1; }","duration":"679.469941ms","start":"2021-11-28T19:17:17.931Z","end":"2021-11-28T19:17:18.611Z","steps":["trace[1679139445] 'process raft request'  (duration: 201.811698ms)","trace[1679139445] 'compare'  (duration: 415.006376ms)","trace[1679139445] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-system/cert-manager-controller; req_size:514; } (duration: 10.760209ms)"],"step_count":3}
{"level":"warn","ts":"2021-11-28T19:17:18.635Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:17.931Z","time spent":"703.772527ms","remote":"127.0.0.1:40700","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":517,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/cert-manager-controller\" mod_revision:82385 > success:<request_put:<key:\"/registry/leases/kube-system/cert-manager-controller\" value_size:457 >> failure:<request_range:<key:\"/registry/leases/kube-system/cert-manager-controller\" > >"}
{"level":"info","ts":"2021-11-28T19:17:18.639Z","caller":"traceutil/trace.go:171","msg":"trace[421843084] linearizableReadLoop","detail":"{readStateIndex:99014; appliedIndex:99012; }","duration":"611.263776ms","start":"2021-11-28T19:17:18.027Z","end":"2021-11-28T19:17:18.639Z","steps":["trace[421843084] 'read index received'  (duration: 105.805548ms)","trace[421843084] 'applied index is now lower than readState.Index'  (duration: 505.456557ms)"],"step_count":2}
{"level":"info","ts":"2021-11-28T19:17:18.642Z","caller":"traceutil/trace.go:171","msg":"trace[637979143] transaction","detail":"{read_only:false; response_revision:82390; number_of_response:1; }","duration":"622.529869ms","start":"2021-11-28T19:17:18.017Z","end":"2021-11-28T19:17:18.640Z","steps":["trace[637979143] 'process raft request'  (duration: 581.445492ms)","trace[637979143] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:116; } (duration: 12.587151ms)","trace[637979143] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:116; } (duration: 26.411311ms)"],"step_count":3}
{"level":"warn","ts":"2021-11-28T19:17:18.654Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.017Z","time spent":"626.141712ms","remote":"127.0.0.1:40646","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":118,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:82386 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128009312824238199 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2021-11-28T19:17:18.672Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"644.497199ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2021-11-28T19:17:18.674Z","caller":"traceutil/trace.go:171","msg":"trace[854971871] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:82390; }","duration":"644.610296ms","start":"2021-11-28T19:17:18.027Z","end":"2021-11-28T19:17:18.672Z","steps":["trace[854971871] 'agreement among raft nodes before linearized reading'  (duration: 611.709991ms)","trace[854971871] 'count revisions from in-memory index tree'  (duration: 32.745973ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:18.681Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.027Z","time spent":"653.959114ms","remote":"127.0.0.1:45390","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":1,"response size":32,"request content":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true "}
{"level":"warn","ts":"2021-11-28T19:17:18.703Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"209.935271ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T19:17:18.703Z","caller":"traceutil/trace.go:171","msg":"trace[235500001] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:82390; }","duration":"228.292126ms","start":"2021-11-28T19:17:18.475Z","end":"2021-11-28T19:17:18.703Z","steps":["trace[235500001] 'agreement among raft nodes before linearized reading'  (duration: 209.860233ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:17:18.705Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"227.550379ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-ba929569-2bfd-4fea-94d9-b734b125d5f6\" ","response":"range_response_count:1 size:1133"}
{"level":"warn","ts":"2021-11-28T19:17:18.766Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"295.874431ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-c3348d3f-8023-4b23-a4fb-64daa8dea44a\" ","response":"range_response_count:1 size:1137"}
{"level":"info","ts":"2021-11-28T19:17:18.766Z","caller":"traceutil/trace.go:171","msg":"trace[275548887] range","detail":"{range_begin:/registry/persistentvolumes/pvc-c3348d3f-8023-4b23-a4fb-64daa8dea44a; range_end:; response_count:1; response_revision:82390; }","duration":"295.966387ms","start":"2021-11-28T19:17:18.470Z","end":"2021-11-28T19:17:18.766Z","steps":["trace[275548887] 'agreement among raft nodes before linearized reading'  (duration: 295.786112ms)"],"step_count":1}
{"level":"info","ts":"2021-11-28T19:17:18.768Z","caller":"traceutil/trace.go:171","msg":"trace[730117808] range","detail":"{range_begin:/registry/persistentvolumes/pvc-ba929569-2bfd-4fea-94d9-b734b125d5f6; range_end:; response_count:1; response_revision:82390; }","duration":"227.642545ms","start":"2021-11-28T19:17:18.478Z","end":"2021-11-28T19:17:18.706Z","steps":["trace[730117808] 'agreement among raft nodes before linearized reading'  (duration: 217.995698ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:17:18.772Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"731.968989ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-cb28b7d5-fe2f-410e-8541-6868a78dc16d\" ","response":"range_response_count:1 size:1137"}
{"level":"warn","ts":"2021-11-28T19:17:18.773Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"298.00676ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-4b80c8fe-6870-44c0-8561-0a8033922e01\" ","response":"range_response_count:1 size:1133"}
{"level":"info","ts":"2021-11-28T19:17:18.773Z","caller":"traceutil/trace.go:171","msg":"trace[340905804] range","detail":"{range_begin:/registry/persistentvolumes/pvc-4b80c8fe-6870-44c0-8561-0a8033922e01; range_end:; response_count:1; response_revision:82390; }","duration":"298.091632ms","start":"2021-11-28T19:17:18.475Z","end":"2021-11-28T19:17:18.773Z","steps":["trace[340905804] 'agreement among raft nodes before linearized reading'  (duration: 168.828115ms)","trace[340905804] 'range keys from in-memory index tree'  (duration: 129.078382ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:18.774Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"521.565769ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T19:17:18.773Z","caller":"traceutil/trace.go:171","msg":"trace[1352067912] range","detail":"{range_begin:/registry/persistentvolumes/pvc-cb28b7d5-fe2f-410e-8541-6868a78dc16d; range_end:; response_count:1; response_revision:82390; }","duration":"732.07157ms","start":"2021-11-28T19:17:18.040Z","end":"2021-11-28T19:17:18.772Z","steps":["trace[1352067912] 'agreement among raft nodes before linearized reading'  (duration: 602.257028ms)","trace[1352067912] 'range keys from in-memory index tree'  (duration: 128.207776ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:18.775Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.040Z","time spent":"734.875291ms","remote":"127.0.0.1:40934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1161,"request content":"key:\"/registry/persistentvolumes/pvc-cb28b7d5-fe2f-410e-8541-6868a78dc16d\" "}
{"level":"info","ts":"2021-11-28T19:17:18.774Z","caller":"traceutil/trace.go:171","msg":"trace[1089808968] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:82390; }","duration":"521.625449ms","start":"2021-11-28T19:17:18.252Z","end":"2021-11-28T19:17:18.774Z","steps":["trace[1089808968] 'agreement among raft nodes before linearized reading'  (duration: 521.544384ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:17:18.775Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.252Z","time spent":"522.566685ms","remote":"127.0.0.1:40780","response type":"/etcdserverpb.KV/Range","request count":0,"request size":90,"response count":0,"response size":30,"request content":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true "}
{"level":"warn","ts":"2021-11-28T19:17:18.776Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"427.843617ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T19:17:18.776Z","caller":"traceutil/trace.go:171","msg":"trace[744159130] range","detail":"{range_begin:/registry/poddisruptionbudgets/; range_end:/registry/poddisruptionbudgets0; response_count:0; response_revision:82390; }","duration":"427.933486ms","start":"2021-11-28T19:17:18.348Z","end":"2021-11-28T19:17:18.776Z","steps":["trace[744159130] 'agreement among raft nodes before linearized reading'  (duration: 427.27627ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:17:18.777Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.348Z","time spent":"428.383989ms","remote":"127.0.0.1:40726","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":0,"response size":30,"request content":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true "}
{"level":"warn","ts":"2021-11-28T19:17:18.777Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"677.711696ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-11-28T19:17:18.777Z","caller":"traceutil/trace.go:171","msg":"trace[2124892461] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:82390; }","duration":"677.757038ms","start":"2021-11-28T19:17:18.099Z","end":"2021-11-28T19:17:18.777Z","steps":["trace[2124892461] 'agreement among raft nodes before linearized reading'  (duration: 676.84668ms)"],"step_count":1}
{"level":"warn","ts":"2021-11-28T19:17:18.779Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.099Z","time spent":"678.670401ms","remote":"127.0.0.1:45440","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":30,"request content":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true "}
{"level":"warn","ts":"2021-11-28T19:17:18.971Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"478.384642ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-b4ae85de-7238-462f-a9ca-7f3974636e0b\" ","response":"range_response_count:1 size:1132"}
{"level":"info","ts":"2021-11-28T19:17:18.972Z","caller":"traceutil/trace.go:171","msg":"trace[994772512] range","detail":"{range_begin:/registry/persistentvolumes/pvc-b4ae85de-7238-462f-a9ca-7f3974636e0b; range_end:; response_count:1; response_revision:82390; }","duration":"478.491668ms","start":"2021-11-28T19:17:18.492Z","end":"2021-11-28T19:17:18.971Z","steps":["trace[994772512] 'agreement among raft nodes before linearized reading'  (duration: 162.506264ms)","trace[994772512] 'range keys from in-memory index tree'  (duration: 308.940743ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:18.972Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.492Z","time spent":"480.471996ms","remote":"127.0.0.1:40934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1156,"request content":"key:\"/registry/persistentvolumes/pvc-b4ae85de-7238-462f-a9ca-7f3974636e0b\" "}
{"level":"warn","ts":"2021-11-28T19:17:18.992Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"574.781245ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c\" ","response":"range_response_count:1 size:1136"}
{"level":"info","ts":"2021-11-28T19:17:18.992Z","caller":"traceutil/trace.go:171","msg":"trace[1526169675] range","detail":"{range_begin:/registry/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c; range_end:; response_count:1; response_revision:82390; }","duration":"577.591498ms","start":"2021-11-28T19:17:18.414Z","end":"2021-11-28T19:17:18.992Z","steps":["trace[1526169675] 'agreement among raft nodes before linearized reading'  (duration: 293.542848ms)","trace[1526169675] 'range keys from bolt db'  (duration: 281.175922ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:18.992Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:18.414Z","time spent":"577.703214ms","remote":"127.0.0.1:40934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1160,"request content":"key:\"/registry/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c\" "}
{"level":"warn","ts":"2021-11-28T19:17:46.373Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"334.416114ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 ","response":"range_response_count:322 size:236200"}
{"level":"info","ts":"2021-11-28T19:17:46.382Z","caller":"traceutil/trace.go:171","msg":"trace[655889195] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:322; response_revision:82402; }","duration":"346.477472ms","start":"2021-11-28T19:17:46.033Z","end":"2021-11-28T19:17:46.379Z","steps":["trace[655889195] 'range keys from in-memory index tree'  (duration: 150.653108ms)","trace[655889195] 'range keys from bolt db'  (duration: 181.504193ms)"],"step_count":2}
{"level":"warn","ts":"2021-11-28T19:17:46.383Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-11-28T19:17:46.033Z","time spent":"349.965338ms","remote":"127.0.0.1:40650","response type":"/etcdserverpb.KV/Range","request count":0,"request size":41,"response count":322,"response size":236224,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 "}

* 
* ==> kernel <==
*  19:17:47 up 17 min,  0 users,  load average: 8.23, 13.14, 12.84
Linux minikube 5.4.0-90-generic #101-Ubuntu SMP Fri Oct 15 20:00:55 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [6947bbacb1e6] <==
* Trace[680679184]: ---"Transformed response object" 778ms (19:16:58.724)
Trace[680679184]: [1.400341757s] [1.400341757s] END
I1128 19:17:00.914054       1 trace.go:205] Trace[888572781]: "List" url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:121066dd-1ff8-4bde-a611-bbf09f41502f,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:16:59.642) (total time: 1269ms):
Trace[888572781]: ---"Writing http response done" count:8 1266ms (19:17:00.912)
Trace[888572781]: [1.269915109s] [1.269915109s] END
I1128 19:17:08.490579       1 trace.go:205] Trace[1413340756]: "List" url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:2e666244-02ac-4bdf-8d84-51bfd3f2aebb,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:06.211) (total time: 2279ms):
Trace[1413340756]: ---"Writing http response done" count:8 2271ms (19:17:08.490)
Trace[1413340756]: [2.279150835s] [2.279150835s] END
I1128 19:17:09.249265       1 trace.go:205] Trace[614407793]: "List" url:/api/v1/secrets,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:2a38e13a-7202-4ff6-a128-1043ca5e9973,client:172.17.0.8,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:08.631) (total time: 617ms):
Trace[614407793]: ---"Writing http response done" count:55 611ms (19:17:09.249)
Trace[614407793]: [617.533995ms] [617.533995ms] END
I1128 19:17:12.576085       1 trace.go:205] Trace[192386278]: "List" url:/api/v1/pods,user-agent:kubectl/v1.22.3 (linux/amd64) kubernetes/c920368,audit-id:e7fd4ec0-0469-4e7d-9c5e-75bc93ae0e73,client:127.0.0.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:12.052) (total time: 516ms):
Trace[192386278]: ---"Listing from storage done" 431ms (19:17:12.483)
Trace[192386278]: [516.78858ms] [516.78858ms] END
I1128 19:17:13.731640       1 trace.go:205] Trace[849422997]: "List etcd3" key:/events,resourceVersion:,resourceVersionMatch:,limit:500,continue: (28-Nov-2021 19:17:12.655) (total time: 1076ms):
Trace[849422997]: [1.076275863s] [1.076275863s] END
I1128 19:17:13.735452       1 trace.go:205] Trace[348830799]: "List" url:/api/v1/events,user-agent:kubectl/v1.22.3 (linux/amd64) kubernetes/c920368,audit-id:54fdcba2-1231-4191-9062-00e1e60d1a19,client:127.0.0.1,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:12.654) (total time: 1079ms):
Trace[348830799]: ---"Listing from storage done" 1076ms (19:17:13.731)
Trace[348830799]: [1.079019424s] [1.079019424s] END
I1128 19:17:17.550322       1 trace.go:205] Trace[1459084346]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.22.3 (linux/amd64) kubernetes/c920368,audit-id:3a81c626-e159-46da-9960-6c162e4a4289,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:16.662) (total time: 850ms):
Trace[1459084346]: ---"About to write a response" 829ms (19:17:17.492)
Trace[1459084346]: [850.996079ms] [850.996079ms] END
I1128 19:17:17.586962       1 trace.go:205] Trace[289003291]: "GuaranteedUpdate etcd3" type:*core.ConfigMap (28-Nov-2021 19:17:16.527) (total time: 1059ms):
Trace[289003291]: ---"Transaction committed" 1014ms (19:17:17.586)
Trace[289003291]: [1.059482332s] [1.059482332s] END
I1128 19:17:17.587618       1 trace.go:205] Trace[1444147672]: "Update" url:/api/v1/namespaces/kube-system/configmaps/cert-manager-controller,user-agent:cert-manager/v0.0.0 (linux/amd64) kubernetes/$Format/leader-election,audit-id:cab2d24f-1c4d-4e27-9692-ac8abffd0695,client:172.17.0.14,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:16.518) (total time: 1069ms):
Trace[1444147672]: ---"Object stored in database" 1064ms (19:17:17.587)
Trace[1444147672]: [1.069002974s] [1.069002974s] END
I1128 19:17:18.646046       1 trace.go:205] Trace[774505588]: "GuaranteedUpdate etcd3" type:*coordination.Lease (28-Nov-2021 19:17:17.879) (total time: 766ms):
Trace[774505588]: ---"Transaction committed" 746ms (19:17:18.645)
Trace[774505588]: [766.356512ms] [766.356512ms] END
I1128 19:17:18.649448       1 trace.go:205] Trace[1956165358]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/cert-manager-controller,user-agent:cert-manager/v0.0.0 (linux/amd64) kubernetes/$Format/leader-election,audit-id:92b9ead7-b746-430c-8486-68db0b0b6e19,client:172.17.0.14,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:17.819) (total time: 828ms):
Trace[1956165358]: ---"Object stored in database" 768ms (19:17:18.647)
Trace[1956165358]: [828.554947ms] [828.554947ms] END
I1128 19:17:18.660718       1 trace.go:205] Trace[2105056156]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (28-Nov-2021 19:17:17.758) (total time: 897ms):
Trace[2105056156]: ---"initial value restored" 142ms (19:17:17.900)
Trace[2105056156]: ---"Transaction committed" 668ms (19:17:18.655)
Trace[2105056156]: [897.60214ms] [897.60214ms] END
I1128 19:17:18.799638       1 trace.go:205] Trace[1192312646]: "Get" url:/api/v1/persistentvolumes/pvc-cb28b7d5-fe2f-410e-8541-6868a78dc16d,user-agent:kube-controller-manager/v1.22.3 (linux/amd64) kubernetes/c920368/system:serviceaccount:kube-system:persistent-volume-binder,audit-id:c1e78393-117e-4768-8b8f-a05544903e9c,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:18.034) (total time: 765ms):
Trace[1192312646]: ---"About to write a response" 765ms (19:17:18.799)
Trace[1192312646]: [765.407515ms] [765.407515ms] END
I1128 19:17:18.998106       1 trace.go:205] Trace[1723278081]: "Get" url:/api/v1/persistentvolumes/pvc-470a2fc8-76a4-4eec-8c6e-068fb23c710c,user-agent:kube-controller-manager/v1.22.3 (linux/amd64) kubernetes/c920368/system:serviceaccount:kube-system:persistent-volume-binder,audit-id:d5227d02-d91b-48ad-9186-50eca4346eb5,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:18.388) (total time: 609ms):
Trace[1723278081]: ---"About to write a response" 608ms (19:17:18.997)
Trace[1723278081]: [609.673475ms] [609.673475ms] END
I1128 19:17:19.240303       1 trace.go:205] Trace[1556877664]: "Update" url:/api/v1/namespaces/kube-system/configmaps/cert-manager-cainjector-leader-election,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format/leader-election,audit-id:444dfedf-3cad-4ec2-8bed-ed96622baab8,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:17.873) (total time: 1366ms):
Trace[1556877664]: ---"About to convert to expected version" 1195ms (19:17:19.068)
Trace[1556877664]: ---"Object stored in database" 169ms (19:17:19.240)
Trace[1556877664]: [1.366612133s] [1.366612133s] END
I1128 19:17:20.762719       1 trace.go:205] Trace[726185912]: "List" url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:affe6b79-a1ab-4b44-9a6a-c2c31f65986a,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:19.149) (total time: 1608ms):
Trace[726185912]: ---"Writing http response done" count:8 1560ms (19:17:20.758)
Trace[726185912]: [1.608681847s] [1.608681847s] END
I1128 19:17:30.617554       1 trace.go:205] Trace[211167733]: "List" url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:e997df10-7505-485e-b54f-3111d6b25777,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:29.882) (total time: 729ms):
Trace[211167733]: ---"Writing http response done" count:8 723ms (19:17:30.612)
Trace[211167733]: [729.916305ms] [729.916305ms] END
I1128 19:17:37.427575       1 trace.go:205] Trace[741943338]: "List" url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:3595c147-7937-4f86-8757-2ceb3b265b81,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:36.889) (total time: 537ms):
Trace[741943338]: ---"Writing http response done" count:8 536ms (19:17:37.426)
Trace[741943338]: [537.434319ms] [537.434319ms] END
I1128 19:17:45.964151       1 trace.go:205] Trace[2039031906]: "List" url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:cainjector/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:ccb1e382-7daa-4831-8771-153b4bb21501,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (28-Nov-2021 19:17:45.159) (total time: 802ms):
Trace[2039031906]: ---"Writing http response done" count:8 751ms (19:17:45.961)
Trace[2039031906]: [802.009497ms] [802.009497ms] END

* 
* ==> kube-apiserver [7dfc03499083] <==
* W1128 14:03:07.779727       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.782486       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.782486       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.803753       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.803820       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.804059       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.819422       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.823146       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.823218       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.819878       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.823360       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.823275       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.826612       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.827191       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.827598       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.827665       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.827695       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.827767       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.833796       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.823402       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.833360       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.841626       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.844776       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.845086       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.845348       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.848107       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.848789       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.849441       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.849484       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.850626       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.850699       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.848622       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.851631       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.851671       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.851714       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.851915       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.848913       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.852464       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.852569       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.852891       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.852957       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.853206       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.853426       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.854064       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.854142       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.854181       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.849526       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.853976       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.863698       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.869517       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.870732       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.872320       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.872751       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.873054       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.883655       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.883805       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.889926       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.890151       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.893015       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W1128 14:03:07.895846       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-controller-manager [8c54193560a0] <==
* I1128 19:05:07.832923       1 shared_informer.go:247] Caches are synced for ephemeral 
I1128 19:05:07.832962       1 shared_informer.go:247] Caches are synced for GC 
I1128 19:05:07.837251       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I1128 19:05:07.874418       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1128 19:05:07.880094       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I1128 19:05:07.881088       1 shared_informer.go:247] Caches are synced for PVC protection 
I1128 19:05:07.881333       1 shared_informer.go:247] Caches are synced for attach detach 
I1128 19:05:07.936046       1 shared_informer.go:247] Caches are synced for disruption 
I1128 19:05:07.936306       1 disruption.go:371] Sending events to api server.
I1128 19:05:07.947993       1 shared_informer.go:247] Caches are synced for endpoint 
I1128 19:05:17.437946       1 trace.go:205] Trace[1298595666]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:05.905) (total time: 11531ms):
Trace[1298595666]: ---"Objects listed" 11531ms (19:05:17.437)
Trace[1298595666]: [11.531924938s] [11.531924938s] END
E1128 19:05:37.538981       1 shared_informer.go:243] unable to sync caches for garbage collector
E1128 19:05:37.575201       1 garbagecollector.go:242] timed out waiting for dependency graph builder sync during GC sync (attempt 1)
E1128 19:05:38.066638       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1128 19:05:38.075400       1 shared_informer.go:240] Waiting for caches to sync for resource quota
W1128 19:05:38.275861       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1128 19:05:38.291563       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I1128 19:05:38.571588       1 trace.go:205] Trace[1804159970]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:05.968) (total time: 32602ms):
Trace[1804159970]: ---"Objects listed" 32602ms (19:05:38.571)
Trace[1804159970]: [32.602694431s] [32.602694431s] END
I1128 19:05:43.302283       1 trace.go:205] Trace[685816324]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:05.969) (total time: 37301ms):
Trace[685816324]: ---"Objects listed" 37062ms (19:05:43.031)
Trace[685816324]: [37.301525379s] [37.301525379s] END
I1128 19:05:43.843113       1 trace.go:205] Trace[1932388359]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:06.037) (total time: 37805ms):
Trace[1932388359]: ---"Objects listed" 37805ms (19:05:43.843)
Trace[1932388359]: [37.805672333s] [37.805672333s] END
I1128 19:05:43.881411       1 trace.go:205] Trace[869719560]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:05.969) (total time: 37911ms):
Trace[869719560]: ---"Objects listed" 37911ms (19:05:43.881)
Trace[869719560]: [37.911832804s] [37.911832804s] END
I1128 19:05:44.078084       1 trace.go:205] Trace[654490707]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:05.969) (total time: 38108ms):
Trace[654490707]: ---"Objects listed" 38108ms (19:05:44.077)
Trace[654490707]: [38.108738303s] [38.108738303s] END
I1128 19:05:44.199946       1 trace.go:205] Trace[619104400]: "Reflector ListAndWatch" name:k8s.io/client-go/metadata/metadatainformer/informer.go:90 (28-Nov-2021 19:05:05.924) (total time: 38275ms):
Trace[619104400]: ---"Objects listed" 38275ms (19:05:44.199)
Trace[619104400]: [38.275179161s] [38.275179161s] END
I1128 19:05:44.228206       1 shared_informer.go:247] Caches are synced for garbage collector 
I1128 19:05:44.237878       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1128 19:05:44.260852       1 shared_informer.go:247] Caches are synced for resource quota 
I1128 19:05:44.276977       1 shared_informer.go:247] Caches are synced for resource quota 
I1128 19:05:44.293650       1 shared_informer.go:247] Caches are synced for garbage collector 
E1128 19:05:44.589213       1 memcache.go:196] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E1128 19:05:44.631591       1 memcache.go:101] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E1128 19:06:59.276451       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:06:59.629098       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 19:07:51.580919       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:07:52.578687       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 19:08:30.872439       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:08:31.904795       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 19:09:01.060008       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:09:01.949829       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 19:09:31.246029       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:09:32.079737       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 19:10:22.548357       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:10:22.910665       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 19:11:13.675935       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 19:11:14.361845       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1128 19:13:36.758836       1 event.go:291] "Event occurred" object="default/hello-node-7567d9fdc9" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hello-node-7567d9fdc9-mpmnm"
I1128 19:15:54.741555       1 request.go:665] Waited for 1.975548373s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/api?timeout=32s

* 
* ==> kube-controller-manager [a33d61e38561] <==
* I1128 13:46:20.878994       1 event.go:291] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/kubernetes-dashboard: Operation cannot be fulfilled on endpoints \"kubernetes-dashboard\": the object has been modified; please apply your changes to the latest version and try again"
E1128 13:46:21.196569       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1128 13:46:21.226082       1 event.go:291] "Event occurred" object="lens-metrics/node-exporter" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint lens-metrics/node-exporter: Operation cannot be fulfilled on endpoints \"node-exporter\": the object has been modified; please apply your changes to the latest version and try again"
W1128 13:46:21.343006       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1128 13:46:21.683597       1 event.go:291] "Event occurred" object="lens-metrics/prometheus" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint lens-metrics/prometheus: Operation cannot be fulfilled on endpoints \"prometheus\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:22.164883       1 event.go:291] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/kubernetes-dashboard: Operation cannot be fulfilled on endpoints \"kubernetes-dashboard\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:22.253717       1 event.go:291] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/dashboard-metrics-scraper: Operation cannot be fulfilled on endpoints \"dashboard-metrics-scraper\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:22.348427       1 event.go:291] "Event occurred" object="default/kube-state-metrics-1638013120" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint default/kube-state-metrics-1638013120: Operation cannot be fulfilled on endpoints \"kube-state-metrics-1638013120\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:23.793281       1 event.go:291] "Event occurred" object="lens-metrics/node-exporter" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint lens-metrics/node-exporter: Operation cannot be fulfilled on endpoints \"node-exporter\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:24.253396       1 event.go:291] "Event occurred" object="lens-metrics/prometheus" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint lens-metrics/prometheus: Operation cannot be fulfilled on endpoints \"prometheus\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:24.731427       1 event.go:291] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/kubernetes-dashboard: Operation cannot be fulfilled on endpoints \"kubernetes-dashboard\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:27.383678       1 event.go:291] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/dashboard-metrics-scraper: Operation cannot be fulfilled on endpoints \"dashboard-metrics-scraper\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:27.656580       1 event.go:291] "Event occurred" object="default/kube-state-metrics-1638013120" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint default/kube-state-metrics-1638013120: Operation cannot be fulfilled on endpoints \"kube-state-metrics-1638013120\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:28.920718       1 event.go:291] "Event occurred" object="lens-metrics/node-exporter" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint lens-metrics/node-exporter: Operation cannot be fulfilled on endpoints \"node-exporter\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:29.383973       1 event.go:291] "Event occurred" object="lens-metrics/prometheus" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint lens-metrics/prometheus: Operation cannot be fulfilled on endpoints \"prometheus\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:29.863813       1 event.go:291] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/kubernetes-dashboard: Operation cannot be fulfilled on endpoints \"kubernetes-dashboard\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:37.644463       1 event.go:291] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/dashboard-metrics-scraper: Operation cannot be fulfilled on endpoints \"dashboard-metrics-scraper\": the object has been modified; please apply your changes to the latest version and try again"
I1128 13:46:37.900237       1 event.go:291] "Event occurred" object="default/kube-state-metrics-1638013120" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint default/kube-state-metrics-1638013120: Operation cannot be fulfilled on endpoints \"kube-state-metrics-1638013120\": the object has been modified; please apply your changes to the latest version and try again"
E1128 13:46:51.211849       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:46:51.374905       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:47:21.259998       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:47:21.399912       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:47:51.274848       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:47:51.422793       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:48:21.340102       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:48:21.448472       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:48:51.479226       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:48:51.521030       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:49:21.529236       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:49:21.567850       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:49:51.546541       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:49:51.592310       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:50:21.580087       1 resource_quota_controller.go:413] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1128 13:50:21.634656       1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1128 13:58:59.785441       1 resource_quota_controller.go:413] failed to discover resources: Get "https://192.168.49.2:8443/api?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
W1128 13:58:59.788567       1 garbagecollector.go:705] failed to discover preferred resources: Get "https://192.168.49.2:8443/api?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
I1128 13:59:46.297142       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I1128 13:59:49.502747       1 event.go:291] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-654cf69797-kv8sm" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:49.903275       1 event.go:291] "Event occurred" object="kube-system/kube-scheduler-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:50.373300       1 event.go:291] "Event occurred" object="kube-system/kube-apiserver-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:50.595162       1 event.go:291] "Event occurred" object="default/cert-manager-1638015062-controller-5d54797847-w8jnf" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:50.828810       1 event.go:291] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5594458c94-tmh5q" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:51.197862       1 event.go:291] "Event occurred" object="lens-metrics/node-exporter-svv24" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:51.913827       1 event.go:291] "Event occurred" object="default/netdata-parent-c5b7cbb99-8f2kt" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:52.502578       1 event.go:291] "Event occurred" object="kube-system/etcd-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:52.614766       1 event.go:291] "Event occurred" object="kube-system/kube-controller-manager-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:52.800024       1 event.go:291] "Event occurred" object="default/cert-manager-1638015062-cainjector-8658ffc775-cw2ng" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:53.006357       1 event.go:291] "Event occurred" object="default/kube-state-metrics-1638013120-67d55bd875-kkq9r" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W1128 13:59:53.181445       1 controller_utils.go:148] Failed to update status for pod "cert-manager-1638015062-webhook-7ccbf994c-v6fbb_default(0e40e330-87bd-4f0b-b2f7-a3d05992d857)": Operation cannot be fulfilled on pods "cert-manager-1638015062-webhook-7ccbf994c-v6fbb": the object has been modified; please apply your changes to the latest version and try again
I1128 13:59:53.181660       1 event.go:291] "Event occurred" object="default/cert-manager-1638015062-webhook-7ccbf994c-v6fbb" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:53.306783       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-dnt6d" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:53.507792       1 event.go:291] "Event occurred" object="lens-metrics/kube-state-metrics-78596b555-7wt4g" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:53.772993       1 event.go:291] "Event occurred" object="lens-metrics/prometheus-0" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:54.017733       1 event.go:291] "Event occurred" object="default/netdata-child-m97rm" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:54.171507       1 event.go:291] "Event occurred" object="kube-system/kube-proxy-t5sgt" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:54.262516       1 event.go:291] "Event occurred" object="default/hello-node-7567d9fdc9-kbfc7" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1128 13:59:54.314981       1 node_lifecycle_controller.go:844] unable to mark all pods NotReady on node minikube: Operation cannot be fulfilled on pods "cert-manager-1638015062-webhook-7ccbf994c-v6fbb": the object has been modified; please apply your changes to the latest version and try again; queuing for retry
I1128 13:59:54.315598       1 node_lifecycle_controller.go:1164] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1128 13:59:54.316597       1 event.go:291] "Event occurred" object="default/sal-ubuntu" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1128 13:59:59.318753       1 node_lifecycle_controller.go:1191] Controller detected that some Nodes are Ready. Exiting master disruption mode.

* 
* ==> kube-proxy [889bed6b15fe] <==
* I1128 19:05:10.483564       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I1128 19:05:10.483688       1 server_others.go:140] Detected node IP 192.168.49.2
W1128 19:05:10.483716       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I1128 19:05:11.141527       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I1128 19:05:11.141585       1 server_others.go:212] Using iptables Proxier.
I1128 19:05:11.141620       1 server_others.go:219] creating dualStackProxier for iptables.
W1128 19:05:11.141642       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I1128 19:05:11.146467       1 server.go:649] Version: v1.22.3
I1128 19:05:11.157990       1 config.go:224] Starting endpoint slice config controller
I1128 19:05:11.160317       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1128 19:05:11.158631       1 config.go:315] Starting service config controller
I1128 19:05:11.160382       1 shared_informer.go:240] Waiting for caches to sync for service config
I1128 19:05:11.460492       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I1128 19:05:11.661843       1 shared_informer.go:247] Caches are synced for service config 
I1128 19:06:14.650184       1 trace.go:205] Trace[986880632]: "iptables ChainExists" (28-Nov-2021 19:06:11.335) (total time: 3190ms):
Trace[986880632]: [3.190410427s] [3.190410427s] END
I1128 19:06:14.651509       1 trace.go:205] Trace[221053872]: "iptables ChainExists" (28-Nov-2021 19:06:11.338) (total time: 3187ms):
Trace[221053872]: [3.187654737s] [3.187654737s] END
I1128 19:06:54.181241       1 trace.go:205] Trace[3658668]: "iptables ChainExists" (28-Nov-2021 19:06:45.710) (total time: 8448ms):
Trace[3658668]: [8.448281451s] [8.448281451s] END
I1128 19:06:54.332850       1 trace.go:205] Trace[1928509662]: "iptables ChainExists" (28-Nov-2021 19:06:45.710) (total time: 8622ms):
Trace[1928509662]: [8.62205493s] [8.62205493s] END
I1128 19:07:47.633738       1 trace.go:205] Trace[1534602043]: "iptables ChainExists" (28-Nov-2021 19:07:44.746) (total time: 2318ms):
Trace[1534602043]: [2.318577752s] [2.318577752s] END
W1128 19:10:17.608756       1 reflector.go:441] k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1128 19:10:17.608776       1 reflector.go:441] k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1128 19:10:17.695729       1 trace.go:205] Trace[1283818387]: "iptables ChainExists" (28-Nov-2021 19:10:12.283) (total time: 5179ms):
Trace[1283818387]: [5.179525253s] [5.179525253s] END
I1128 19:10:17.688317       1 trace.go:205] Trace[219954707]: "iptables ChainExists" (28-Nov-2021 19:10:12.289) (total time: 5112ms):
Trace[219954707]: [5.112853598s] [5.112853598s] END
I1128 19:10:43.401708       1 trace.go:205] Trace[918781112]: "iptables save" (28-Nov-2021 19:10:34.961) (total time: 6387ms):
Trace[918781112]: [6.3874315s] [6.3874315s] END
I1128 19:16:13.911705       1 trace.go:205] Trace[354111834]: "iptables ChainExists" (28-Nov-2021 19:16:11.340) (total time: 2556ms):
Trace[354111834]: [2.556322754s] [2.556322754s] END
I1128 19:16:14.386691       1 trace.go:205] Trace[1333070782]: "iptables ChainExists" (28-Nov-2021 19:16:12.027) (total time: 2358ms):
Trace[1333070782]: [2.358805281s] [2.358805281s] END

* 
* ==> kube-proxy [ea9e64ce2461] <==
* I1128 13:39:48.269387       1 trace.go:205] Trace[1869291595]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:39:22.683) (total time: 25570ms):
Trace[1869291595]: [25.570560026s] [25.570560026s] END
E1128 13:39:48.272142       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": net/http: TLS handshake timeout
I1128 13:39:52.476446       1 trace.go:205] Trace[1230264280]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:39:22.454) (total time: 30017ms):
Trace[1230264280]: [30.01764452s] [30.01764452s] END
E1128 13:39:52.483550       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": net/http: TLS handshake timeout
I1128 13:40:25.425481       1 trace.go:205] Trace[2144343317]: "iptables ChainExists" (28-Nov-2021 13:40:06.917) (total time: 18458ms):
Trace[2144343317]: [18.458873464s] [18.458873464s] END
I1128 13:40:26.491160       1 trace.go:205] Trace[1780467624]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:40:07.851) (total time: 18639ms):
Trace[1780467624]: [18.639914448s] [18.639914448s] END
E1128 13:40:26.492928       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": net/http: TLS handshake timeout
I1128 13:40:40.293071       1 trace.go:205] Trace[328292844]: "iptables ChainExists" (28-Nov-2021 13:40:34.721) (total time: 5466ms):
Trace[328292844]: [5.466563123s] [5.466563123s] END
I1128 13:40:40.721760       1 trace.go:205] Trace[224592057]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:40:19.443) (total time: 21275ms):
Trace[224592057]: [21.275803362s] [21.275803362s] END
E1128 13:40:40.742897       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": net/http: TLS handshake timeout
I1128 13:41:08.678265       1 trace.go:205] Trace[1438196684]: "iptables ChainExists" (28-Nov-2021 13:40:34.744) (total time: 10856ms):
Trace[1438196684]: [10.856985355s] [10.856985355s] END
I1128 13:41:18.761431       1 trace.go:205] Trace[1191342819]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:41:08.620) (total time: 10127ms):
Trace[1191342819]: [10.127819833s] [10.127819833s] END
E1128 13:41:18.761748       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:41:25.011571       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:42:30.177518       1 trace.go:205] Trace[477334894]: "iptables ChainExists" (28-Nov-2021 13:42:12.023) (total time: 18150ms):
Trace[477334894]: [18.150351947s] [18.150351947s] END
I1128 13:42:48.738284       1 trace.go:205] Trace[208157585]: "iptables ChainExists" (28-Nov-2021 13:42:17.305) (total time: 31416ms):
Trace[208157585]: [31.416549372s] [31.416549372s] END
I1128 13:42:48.739013       1 trace.go:205] Trace[1231669210]: "iptables ChainExists" (28-Nov-2021 13:42:37.219) (total time: 11509ms):
Trace[1231669210]: [11.509881151s] [11.509881151s] END
I1128 13:45:20.234713       1 trace.go:205] Trace[321271439]: "iptables ChainExists" (28-Nov-2021 13:44:46.591) (total time: 33499ms):
Trace[321271439]: [33.499827034s] [33.499827034s] END
I1128 13:45:20.235106       1 trace.go:205] Trace[1717489873]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:41:51.084) (total time: 208832ms):
Trace[1717489873]: [3m28.832336102s] [3m28.832336102s] END
I1128 13:45:20.280687       1 trace.go:205] Trace[1822405287]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:42:48.713) (total time: 151466ms):
Trace[1822405287]: [2m31.466177755s] [2m31.466177755s] END
E1128 13:45:20.289055       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": net/http: TLS handshake timeout
I1128 13:45:20.289109       1 trace.go:205] Trace[2145724547]: "iptables ChainExists" (28-Nov-2021 13:44:07.940) (total time: 72262ms):
Trace[2145724547]: [1m12.262529029s] [1m12.262529029s] END
E1128 13:45:20.345393       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": net/http: TLS handshake timeout
E1128 13:45:21.684127       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:21.835488       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:24.550288       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:25.106924       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:28.666777       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:30.123604       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:45:50.626458       1 trace.go:205] Trace[1987037717]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:40.304) (total time: 10321ms):
Trace[1987037717]: [10.321754367s] [10.321754367s] END
E1128 13:45:50.626701       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=80169": net/http: TLS handshake timeout
I1128 13:45:51.261524       1 trace.go:205] Trace[1632255020]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:41.227) (total time: 10034ms):
Trace[1632255020]: [10.03413402s] [10.03413402s] END
E1128 13:45:51.261610       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?labelSelector=%!s(MISSING)ervice.kubernetes.io%!F(MISSING)headless%!C(MISSING)%!s(MISSING)ervice.kubernetes.io%!F(MISSING)service-proxy-name&resourceVersion=79515": net/http: TLS handshake timeout
I1128 13:55:09.704210       1 trace.go:205] Trace[1215134772]: "iptables ChainExists" (28-Nov-2021 13:55:04.475) (total time: 5219ms):
Trace[1215134772]: [5.219513997s] [5.219513997s] END
I1128 13:59:00.278314       1 trace.go:205] Trace[1732074353]: "iptables ChainExists" (28-Nov-2021 13:58:49.628) (total time: 10543ms):
Trace[1732074353]: [10.543381671s] [10.543381671s] END
I1128 13:59:00.278454       1 trace.go:205] Trace[733537702]: "iptables ChainExists" (28-Nov-2021 13:58:49.627) (total time: 10543ms):
Trace[733537702]: [10.543985626s] [10.543985626s] END
I1128 13:59:50.285993       1 trace.go:205] Trace[1943644944]: "iptables ChainExists" (28-Nov-2021 13:59:46.916) (total time: 3275ms):
Trace[1943644944]: [3.275985491s] [3.275985491s] END
I1128 13:59:50.286070       1 trace.go:205] Trace[1955378489]: "iptables ChainExists" (28-Nov-2021 13:59:42.213) (total time: 7981ms):
Trace[1955378489]: [7.981632229s] [7.981632229s] END

* 
* ==> kube-scheduler [377d155d3515] <==
* I1128 13:44:01.488504       1 trace.go:205] Trace[749369232]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:43:18.920) (total time: 42567ms):
Trace[749369232]: [42.567470592s] [42.567470592s] END
E1128 13:44:01.488546       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:08.857718       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=80211": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:44:08.930226       1 trace.go:205] Trace[45072878]: "Reflector ListAndWatch" name:k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205 (28-Nov-2021 13:43:50.717) (total time: 18141ms):
Trace[45072878]: [18.141281721s] [18.141281721s] END
E1128 13:44:08.931684       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&resourceVersion=80251": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:09.244821       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=80167": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:09.245963       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:09.245981       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:44:09.555307       1 trace.go:205] Trace[1305404169]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:43:46.718) (total time: 22831ms):
Trace[1305404169]: [22.831872006s] [22.831872006s] END
E1128 13:44:09.555379       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:44:18.062681       1 trace.go:205] Trace[290204792]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:43:50.964) (total time: 25722ms):
Trace[290204792]: [25.72216509s] [25.72216509s] END
E1128 13:44:18.926824       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:21.130670       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=80211": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:44.663124       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:44.664415       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:44.672428       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:44.679669       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:44.680577       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:54.583344       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=79976": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:54.587785       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=80167": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:54.660055       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:54.701259       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:44:58.487961       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:45:02.449709       1 trace.go:205] Trace[1569812143]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:44:48.016) (total time: 14407ms):
Trace[1569812143]: [14.407290821s] [14.407290821s] END
E1128 13:45:02.452535       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=80211": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:02.441041       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=80171": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:06.066588       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&resourceVersion=80251": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:10.754237       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:15.616519       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=80211": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:15.685695       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:29.860723       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=80211": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:30.179157       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:30.608150       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=80167": dial tcp 192.168.49.2:8443: connect: connection refused
E1128 13:45:31.220224       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?resourceVersion=79515": dial tcp 192.168.49.2:8443: connect: connection refused
I1128 13:45:45.808860       1 trace.go:205] Trace[1900447978]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:35.742) (total time: 10058ms):
Trace[1900447978]: [10.058854125s] [10.058854125s] END
E1128 13:45:45.813913       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=79515": net/http: TLS handshake timeout
I1128 13:45:45.816740       1 trace.go:205] Trace[890992450]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:35.742) (total time: 10063ms):
Trace[890992450]: [10.063623517s] [10.063623517s] END
E1128 13:45:45.816956       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=79515": net/http: TLS handshake timeout
I1128 13:45:45.858185       1 trace.go:205] Trace[1489559762]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:35.743) (total time: 10115ms):
Trace[1489559762]: [10.115032148s] [10.115032148s] END
E1128 13:45:45.864928       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=79976": net/http: TLS handshake timeout
I1128 13:45:46.338453       1 trace.go:205] Trace[1276851437]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:36.256) (total time: 10082ms):
Trace[1276851437]: [10.08210763s] [10.08210763s] END
E1128 13:45:46.338509       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=79515": net/http: TLS handshake timeout
I1128 13:45:49.596188       1 trace.go:205] Trace[68728185]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:39.521) (total time: 10075ms):
Trace[68728185]: [10.075140422s] [10.075140422s] END
E1128 13:45:49.598704       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=80171": net/http: TLS handshake timeout
I1128 13:45:51.286755       1 trace.go:205] Trace[1955211186]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (28-Nov-2021 13:45:40.647) (total time: 10638ms):
Trace[1955211186]: ---"Objects listed" 10552ms (13:45:51.200)
Trace[1955211186]: [10.638793137s] [10.638793137s] END
I1128 14:03:05.306557       1 configmap_cafile_content.go:222] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1128 14:03:05.382220       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1128 14:03:05.409771       1 secure_serving.go:311] Stopped listening on 127.0.0.1:10259

* 
* ==> kube-scheduler [bfc517c58a4f] <==
* I1128 19:03:06.307080       1 serving.go:347] Generated self-signed cert in-memory
W1128 19:03:11.925970       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1128 19:03:11.940803       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1128 19:03:11.941462       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W1128 19:03:11.941558       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1128 19:03:12.258946       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1128 19:03:12.258995       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1128 19:03:12.260361       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I1128 19:03:12.260550       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1128 19:03:12.463907       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
W1128 19:10:17.682986       1 reflector.go:441] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1128 19:11:09.817447       1 reflector.go:441] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding

* 
* ==> kubelet <==
* -- Logs begin at Sun 2021-11-28 19:02:16 UTC, end at Sun 2021-11-28 19:17:48 UTC. --
Nov 28 19:12:31 minikube kubelet[858]: E1128 19:12:31.792852     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-child-m97rm_fb3cc541-2640-4ccc-8589-cb5a139ea879: no such file or directory" pod="default/netdata-child-m97rm"
Nov 28 19:12:31 minikube kubelet[858]: E1128 19:12:31.793857     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-parent-c5b7cbb99-8f2kt_18feb9ae-dd01-4d8c-9f9b-75a7586294ff: no such file or directory" pod="default/netdata-parent-c5b7cbb99-8f2kt"
Nov 28 19:12:33 minikube kubelet[858]: E1128 19:12:33.097171     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-child-m97rm_fb3cc541-2640-4ccc-8589-cb5a139ea879: no such file or directory" pod="default/netdata-child-m97rm"
Nov 28 19:12:33 minikube kubelet[858]: E1128 19:12:33.097518     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-parent-c5b7cbb99-8f2kt_18feb9ae-dd01-4d8c-9f9b-75a7586294ff: no such file or directory" pod="default/netdata-parent-c5b7cbb99-8f2kt"
Nov 28 19:12:43 minikube kubelet[858]: E1128 19:12:43.139682     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-parent-c5b7cbb99-8f2kt_18feb9ae-dd01-4d8c-9f9b-75a7586294ff: no such file or directory" pod="default/netdata-parent-c5b7cbb99-8f2kt"
Nov 28 19:12:43 minikube kubelet[858]: E1128 19:12:43.141814     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-child-m97rm_fb3cc541-2640-4ccc-8589-cb5a139ea879: no such file or directory" pod="default/netdata-child-m97rm"
Nov 28 19:12:43 minikube kubelet[858]: E1128 19:12:43.316982     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-child-m97rm_fb3cc541-2640-4ccc-8589-cb5a139ea879: no such file or directory" pod="default/netdata-child-m97rm"
Nov 28 19:12:43 minikube kubelet[858]: E1128 19:12:43.317269     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-parent-c5b7cbb99-8f2kt_18feb9ae-dd01-4d8c-9f9b-75a7586294ff: no such file or directory" pod="default/netdata-parent-c5b7cbb99-8f2kt"
Nov 28 19:12:43 minikube kubelet[858]: E1128 19:12:43.317694     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-parent-c5b7cbb99-8f2kt_18feb9ae-dd01-4d8c-9f9b-75a7586294ff: no such file or directory" pod="default/netdata-parent-c5b7cbb99-8f2kt"
Nov 28 19:12:43 minikube kubelet[858]: E1128 19:12:43.318020     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-child-m97rm_fb3cc541-2640-4ccc-8589-cb5a139ea879: no such file or directory" pod="default/netdata-child-m97rm"
Nov 28 19:12:53 minikube kubelet[858]: E1128 19:12:53.224685     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-child-m97rm_fb3cc541-2640-4ccc-8589-cb5a139ea879: no such file or directory" pod="default/netdata-child-m97rm"
Nov 28 19:12:53 minikube kubelet[858]: E1128 19:12:53.225869     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_netdata-parent-c5b7cbb99-8f2kt_18feb9ae-dd01-4d8c-9f9b-75a7586294ff: no such file or directory" pod="default/netdata-parent-c5b7cbb99-8f2kt"
Nov 28 19:13:37 minikube kubelet[858]: I1128 19:13:37.057960     858 topology_manager.go:200] "Topology Admit Handler"
Nov 28 19:13:37 minikube kubelet[858]: I1128 19:13:37.272090     858 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x5fjv\" (UniqueName: \"kubernetes.io/projected/3600ace7-7301-4572-b19a-a262b9b29e83-kube-api-access-x5fjv\") pod \"hello-node-7567d9fdc9-mpmnm\" (UID: \"3600ace7-7301-4572-b19a-a262b9b29e83\") "
Nov 28 19:13:38 minikube kubelet[858]: I1128 19:13:38.610876     858 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="2094dedf18350e5d1a112cbe98412a03333c267878dae3c85c3cd946824f82cd"
Nov 28 19:13:38 minikube kubelet[858]: I1128 19:13:38.617955     858 scope.go:110] "RemoveContainer" containerID="952a5cc364d79da2ff8e7ce0e5a96b5fb25190069565187ae1562573a137070b"
Nov 28 19:13:38 minikube kubelet[858]: I1128 19:13:38.705926     858 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rv8fg\" (UniqueName: \"kubernetes.io/projected/9be052ce-d6bc-4928-9451-4f333d73502c-kube-api-access-rv8fg\") pod \"9be052ce-d6bc-4928-9451-4f333d73502c\" (UID: \"9be052ce-d6bc-4928-9451-4f333d73502c\") "
Nov 28 19:13:38 minikube kubelet[858]: I1128 19:13:38.748776     858 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/9be052ce-d6bc-4928-9451-4f333d73502c-kube-api-access-rv8fg" (OuterVolumeSpecName: "kube-api-access-rv8fg") pod "9be052ce-d6bc-4928-9451-4f333d73502c" (UID: "9be052ce-d6bc-4928-9451-4f333d73502c"). InnerVolumeSpecName "kube-api-access-rv8fg". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 28 19:13:38 minikube kubelet[858]: I1128 19:13:38.808354     858 reconciler.go:319] "Volume detached for volume \"kube-api-access-rv8fg\" (UniqueName: \"kubernetes.io/projected/9be052ce-d6bc-4928-9451-4f333d73502c-kube-api-access-rv8fg\") on node \"minikube\" DevicePath \"\""
Nov 28 19:13:39 minikube kubelet[858]: I1128 19:13:39.300821     858 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/hello-node-7567d9fdc9-mpmnm through plugin: invalid network status for"
Nov 28 19:13:39 minikube kubelet[858]: I1128 19:13:39.304968     858 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/hello-node-7567d9fdc9-mpmnm through plugin: invalid network status for"
Nov 28 19:13:39 minikube kubelet[858]: I1128 19:13:39.386587     858 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="5eb2223ce9e3d317c149790e27e4f5b58705eb361d156acc70e3d5810dc1af6f"
Nov 28 19:13:40 minikube kubelet[858]: I1128 19:13:40.382278     858 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/hello-node-7567d9fdc9-mpmnm through plugin: invalid network status for"
Nov 28 19:13:41 minikube kubelet[858]: I1128 19:13:41.314823     858 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=9be052ce-d6bc-4928-9451-4f333d73502c path="/var/lib/kubelet/pods/9be052ce-d6bc-4928-9451-4f333d73502c/volumes"
Nov 28 19:13:44 minikube kubelet[858]: W1128 19:13:44.357573     858 conversion.go:111] Could not get instant cpu stats: cumulative stats decrease
Nov 28 19:13:44 minikube kubelet[858]: W1128 19:13:44.366674     858 conversion.go:111] Could not get instant cpu stats: cumulative stats decrease
Nov 28 19:13:44 minikube kubelet[858]: W1128 19:13:44.377533     858 conversion.go:111] Could not get instant cpu stats: cumulative stats decrease
Nov 28 19:14:24 minikube kubelet[858]: I1128 19:14:24.935827     858 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"kube-api-access-x5fjv\" (UniqueName: \"kubernetes.io/projected/3600ace7-7301-4572-b19a-a262b9b29e83-kube-api-access-x5fjv\") pod \"3600ace7-7301-4572-b19a-a262b9b29e83\" (UID: \"3600ace7-7301-4572-b19a-a262b9b29e83\") "
Nov 28 19:14:24 minikube kubelet[858]: I1128 19:14:24.960151     858 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3600ace7-7301-4572-b19a-a262b9b29e83-kube-api-access-x5fjv" (OuterVolumeSpecName: "kube-api-access-x5fjv") pod "3600ace7-7301-4572-b19a-a262b9b29e83" (UID: "3600ace7-7301-4572-b19a-a262b9b29e83"). InnerVolumeSpecName "kube-api-access-x5fjv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 28 19:14:25 minikube kubelet[858]: I1128 19:14:25.037823     858 reconciler.go:319] "Volume detached for volume \"kube-api-access-x5fjv\" (UniqueName: \"kubernetes.io/projected/3600ace7-7301-4572-b19a-a262b9b29e83-kube-api-access-x5fjv\") on node \"minikube\" DevicePath \"\""
Nov 28 19:14:25 minikube kubelet[858]: I1128 19:14:25.709439     858 scope.go:110] "RemoveContainer" containerID="6593f7d97c38b6bb2f119c667de8be3d0cb3e8ad6e98251f271c9699c722acd7"
Nov 28 19:14:27 minikube kubelet[858]: I1128 19:14:27.292687     858 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=3600ace7-7301-4572-b19a-a262b9b29e83 path="/var/lib/kubelet/pods/3600ace7-7301-4572-b19a-a262b9b29e83/volumes"
Nov 28 19:14:31 minikube kubelet[858]: I1128 19:14:31.826396     858 scope.go:110] "RemoveContainer" containerID="1857abbe33604ce9543a1d811b26578bb7b922afd6aa2f4f96328055fccfec1e"
Nov 28 19:14:34 minikube kubelet[858]: E1128 19:14:34.778889     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_hello-node-7567d9fdc9-mpmnm_3600ace7-7301-4572-b19a-a262b9b29e83: no such file or directory" pod="default/hello-node-7567d9fdc9-mpmnm"
Nov 28 19:14:43 minikube kubelet[858]: E1128 19:14:43.322164     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_hello-node-7567d9fdc9-mpmnm_3600ace7-7301-4572-b19a-a262b9b29e83: no such file or directory" pod="default/hello-node-7567d9fdc9-mpmnm"
Nov 28 19:14:43 minikube kubelet[858]: E1128 19:14:43.328633     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_hello-node-7567d9fdc9-mpmnm_3600ace7-7301-4572-b19a-a262b9b29e83: no such file or directory" pod="default/hello-node-7567d9fdc9-mpmnm"
Nov 28 19:14:44 minikube kubelet[858]: E1128 19:14:44.849963     858 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_hello-node-7567d9fdc9-mpmnm_3600ace7-7301-4572-b19a-a262b9b29e83: no such file or directory" pod="default/hello-node-7567d9fdc9-mpmnm"
Nov 28 19:15:55 minikube kubelet[858]: I1128 19:15:55.748948     858 trace.go:205] Trace[1464777764]: "iptables ChainExists" (28-Nov-2021 19:15:53.293) (total time: 2447ms):
Nov 28 19:15:55 minikube kubelet[858]: Trace[1464777764]: [2.447570137s] [2.447570137s] END
Nov 28 19:15:55 minikube kubelet[858]: I1128 19:15:55.780243     858 trace.go:205] Trace[1307859263]: "iptables ChainExists" (28-Nov-2021 19:15:53.339) (total time: 2399ms):
Nov 28 19:15:55 minikube kubelet[858]: Trace[1307859263]: [2.39932397s] [2.39932397s] END
Nov 28 19:15:57 minikube kubelet[858]: I1128 19:15:57.260023     858 scope.go:110] "RemoveContainer" containerID="7721b86b0a3c2dfa1768f36bcf298db0579215de6deea62c59d209fc6fe3b647"
Nov 28 19:15:57 minikube kubelet[858]: I1128 19:15:57.260604     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:15:57 minikube kubelet[858]: E1128 19:15:57.261871     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:16:09 minikube kubelet[858]: I1128 19:16:09.289556     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:16:09 minikube kubelet[858]: E1128 19:16:09.793309     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:16:22 minikube kubelet[858]: I1128 19:16:22.288578     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:16:22 minikube kubelet[858]: E1128 19:16:22.304521     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:16:35 minikube kubelet[858]: I1128 19:16:35.283037     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:16:35 minikube kubelet[858]: E1128 19:16:35.287567     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:16:46 minikube kubelet[858]: I1128 19:16:46.279958     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:16:46 minikube kubelet[858]: E1128 19:16:46.280255     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:17:00 minikube kubelet[858]: I1128 19:17:00.466909     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:17:00 minikube kubelet[858]: E1128 19:17:00.468784     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:17:13 minikube kubelet[858]: I1128 19:17:13.357691     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:17:13 minikube kubelet[858]: E1128 19:17:13.363059     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:17:28 minikube kubelet[858]: I1128 19:17:28.290954     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:17:28 minikube kubelet[858]: E1128 19:17:28.314782     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea
Nov 28 19:17:41 minikube kubelet[858]: I1128 19:17:41.282284     858 scope.go:110] "RemoveContainer" containerID="d2a9807bffa51ddbe5845927a8ca1a396b4414a0a4263632c48fd5239069a881"
Nov 28 19:17:41 minikube kubelet[858]: E1128 19:17:41.285846     858 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d9c9d770-cd9d-4a7c-accd-ed77131f86ea)\"" pod="kube-system/storage-provisioner" podUID=d9c9d770-cd9d-4a7c-accd-ed77131f86ea

* 
* ==> kubernetes-dashboard [5c1aff244410] <==
* 2021/11/28 13:46:18 Using namespace: kubernetes-dashboard
2021/11/28 13:46:18 Using in-cluster config to connect to apiserver
2021/11/28 13:46:18 Using secret token for csrf signing
2021/11/28 13:46:18 Initializing csrf token from kubernetes-dashboard-csrf secret
2021/11/28 13:46:18 Successful initial request to the apiserver, version: v1.22.3
2021/11/28 13:46:18 Generating JWE encryption key
2021/11/28 13:46:18 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2021/11/28 13:46:18 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2021/11/28 13:46:19 Initializing JWE encryption key from synchronized object
2021/11/28 13:46:19 Creating in-cluster Sidecar client
2021/11/28 13:46:19 Serving insecurely on HTTP port: 9090
2021/11/28 13:46:19 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2021/11/28 13:46:49 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2021/11/28 13:47:19 Successful request to sidecar
2021/11/28 13:59:00 Metric client health check failed: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz": net/http: TLS handshake timeout. Retrying in 30 seconds.
2021/11/28 13:59:49 Successful request to sidecar
2021/11/28 13:46:18 Starting overwatch

* 
* ==> kubernetes-dashboard [e3165b893320] <==
* 2021/11/28 19:05:19 Using namespace: kubernetes-dashboard
2021/11/28 19:05:19 Using in-cluster config to connect to apiserver
2021/11/28 19:05:33 Using secret token for csrf signing
2021/11/28 19:05:33 Initializing csrf token from kubernetes-dashboard-csrf secret
2021/11/28 19:05:37 Successful initial request to the apiserver, version: v1.22.3
2021/11/28 19:05:37 Generating JWE encryption key
2021/11/28 19:05:37 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2021/11/28 19:05:37 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2021/11/28 19:05:39 Initializing JWE encryption key from synchronized object
2021/11/28 19:05:39 Creating in-cluster Sidecar client
2021/11/28 19:05:39 Serving insecurely on HTTP port: 9090
2021/11/28 19:05:40 Successful request to sidecar
2021/11/28 19:10:17 Metric client health check failed: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz": http2: client connection lost. Retrying in 30 seconds.
2021/11/28 19:11:13 Successful request to sidecar
2021/11/28 19:05:19 Starting overwatch
I1128 19:11:11.649940       1 request.go:668] Waited for 20.390944277s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz

* 
* ==> storage-provisioner [d2a9807bffa5] <==
* sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0001644c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0001644c0, 0x18b3d60, 0xc00019e090, 0x1, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001644c0, 0x3b9aca00, 0x0, 0x1, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001644c0, 0x3b9aca00, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 78 [sync.Cond.Wait, 4 minutes]:
sync.runtime_notifyListWait(0xc000588250, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000588240)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc00007a1e0, 0x0, 0x0, 0xc000056d00)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc00003c280, 0x18e5530, 0xc0000c6100, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0001644e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0001644e0, 0x18b3d60, 0xc00016e480, 0xc00007a101, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001644e0, 0x3b9aca00, 0x0, 0x1, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001644e0, 0x3b9aca00, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 79 [sync.Cond.Wait, 4 minutes]:
sync.runtime_notifyListWait(0xc000588290, 0x2)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000588280)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc00007a3c0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc00003c280, 0x18e5530, 0xc0000c6100, 0x203001)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000164500)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000164500, 0x18b3d60, 0xc00016e450, 0x1, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000164500, 0x3b9aca00, 0x0, 0x1, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc000164500, 0x3b9aca00, 0xc0005321e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

